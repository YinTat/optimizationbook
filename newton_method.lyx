#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass optbook
\use_default_options true
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding auto-legacy
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Section
The Newton Method
\begin_inset CommandInset label
LatexCommand label
name "sec:Newton-Method"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
mention higher order method here.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We begin with the Newton-Raphson method for finding the zeros of a function 
\begin_inset Formula $g$
\end_inset

,
 i.e.
 finding 
\begin_inset Formula $x$
\end_inset

 such that 
\begin_inset Formula $g(x)=0$
\end_inset

.
 In optimization,
 the goal is to find a root of the gradient,
 
\begin_inset Formula $\nabla f(x)=0$
\end_inset

.
 The Newton-Raphson iteration approximates a function by its gradient/Jacobian,
 then guesses where the gradient intersects the zero line as a likely zero,
 repeating this process until a sufficient approximation has been found.
 In one dimension,
 we approximate the function 
\begin_inset Formula $g$
\end_inset

 by its gradient 
\begin_inset Formula 
\[
g(x)\sim g(x^{(k)})+g'(x^{(k)})(x-x^{(k)}).
\]

\end_inset

Finding the zeros of the right hand side,
 we have the Newton step
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-\frac{g(x^{(k)})}{g'(x^{(k)})}.
\]

\end_inset

In high dimension,
 we can approximate the function by its Jacobian 
\begin_inset Formula $g(x)\sim g(x^{(k)})+Dg(x^{(k)})(x-x^{(k)})$
\end_inset

 and this gives the step
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-\left(Dg(x^{(k)})\right)^{-1}g(x^{(k)}).
\]

\end_inset


\end_layout

\begin_layout Standard
When the function 
\begin_inset Formula $g(x)=\nabla f(x)$
\end_inset

,
 then the Newton step becomes
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-\left(\nabla^{2}f(x^{(k)})\right)^{-1}\nabla f(x^{(k)}).
\]

\end_inset


\end_layout

\begin_layout Standard
Alternatively,
 we can derive the Newton step as 
\begin_inset Formula 
\[
x^{(k+1)}\leftarrow\min_{x}f(x^{(k)})+\left\langle \nabla f(x^{(k)}),x-x^{(k)}\right\rangle +\frac{1}{2}(x-x^{(k)})^{\mathsf{T}}(\nabla^{2}f(x^{(k)}))(x-x^{(k)}).
\]

\end_inset

In words,
 the Newton step finds the minimum of the second order approximation of the function.
 We note that the Newton iteration is a natural idea,
 since
\end_layout

\begin_layout Enumerate
It is affine invariant (changing coordinate systems does not affect convergence).
\end_layout

\begin_layout Enumerate
It is the fastest descent direction taking the Hessian into account.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
To see why affine-invariance is important for optimization,
 we consider the following function
\begin_inset Formula 
\[
f(x_{1},x_{2})=\frac{100}{2}x_{1}^{2}+\frac{1}{2}x_{2}^{2}.
\]

\end_inset

The gradient descent for this function is
\begin_inset Formula 
\[
(x_{1},x_{2})\leftarrow(x_{1},x_{2})-h\nabla f(x_{1},x_{2})=((1-100h)x_{1},(1-h)x_{2}).
\]

\end_inset

We need 
\begin_inset Formula $h<\frac{1}{100}$
\end_inset

 in order for the first coordinate to converge,
 but this will make the second coordinate converge too slowly.
 In general,
 we may want to take different step sizes for different directions and Newton method gives the best step if the function is quadratic.
\end_layout

\begin_layout Standard
For many classes of functions,
 gradient methods converge to the solution linearly (namely,
 it takes 
\begin_inset Formula $c\cdot\log\frac{1}{\epsilon}$
\end_inset

 iterations for some 
\begin_inset Formula $c$
\end_inset

 depending on the problem) while the Newton method converges to the solution quadratically (namely,
 it takes 
\begin_inset Formula $c'\cdot\log\log\frac{1}{\epsilon}$
\end_inset

 for some 
\begin_inset Formula $c'$
\end_inset

) if the starting point is sufficiently close to a root.
 However,
 each step of Newton method involves solving a linear system,
 which can be much more expensive.
 Furthermore,
 Newton method may not converges if the starting point is far away.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithm2e}[H]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
caption{
\end_layout

\end_inset


\begin_inset Formula $\mathtt{NewtonMethod}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
SetAlgoLined
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Input:

\series default
 Initial point 
\begin_inset Formula $x^{(0)}\in\Rn$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
For{
\end_layout

\end_inset


\begin_inset Formula $k=0,1,\cdots,T-1$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}{
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $x^{(k+1)}=x^{(k)}-\left(Dg(x^{(k)})\right)^{-1}g(x^{(k)}).$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Return
\end_layout

\end_inset

 
\begin_inset Formula $x^{(T)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithm2e}
\end_layout

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Quadratic convergence
\end_layout

\end_inset

Assume that 
\begin_inset Formula $g:\Rn\rightarrow\Rn$
\end_inset

 is twice continuously differentiable.
 Let 
\begin_inset Formula $x^{(k)}$
\end_inset

 be the sequence given by the Newton method.
 Suppose that 
\begin_inset Formula $x^{(k)}$
\end_inset

 converges to some 
\begin_inset Formula $x^{*}$
\end_inset

 such that 
\begin_inset Formula $g(x^{*})=0$
\end_inset

 and 
\begin_inset Formula $Dg(x^{*})$
\end_inset

 is invertible.
 Then,
 for 
\begin_inset Formula $k$
\end_inset

 large enough,
 we have
\begin_inset Note Note
status open

\begin_layout Plain Layout
we need bound on operator norm below for every 
\begin_inset Formula $x^{(k)}$
\end_inset

right?
 not just 
\begin_inset Formula $x^{*}?$
\end_inset

 We are assuming 
\begin_inset Formula $g$
\end_inset

 is twice continuously differentiable.
 So,
 they are same.
 We already assume 
\begin_inset Formula $x^{(k)}$
\end_inset

 converges.
\end_layout

\end_inset


\begin_inset Formula 
\[
\|x^{(k+1)}-x^{*}\|\leq\|(Dg(x^{*}))^{-1}D^{2}g(x^{*})\|_{\op}\cdot\|x^{(k)}-x^{*}\|^{2}.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
should change this lemma to affine invariant version.
 Measure distance by the Jacobian norm.
\end_layout

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $e^{(k)}=x^{*}-x^{(k)}$
\end_inset

.
 Then,
 we have that
\begin_inset Formula 
\[
g(x^{*})=g(x^{(k)})+Dg(x^{(k)})[e^{(k)}]+\int_{0}^{1}(1-s)D^{2}g(x^{(k)}+se^{(k)})[e^{(k)},e^{(k)}]ds.
\]

\end_inset

Hence,
 we have
\begin_inset Formula 
\[
0=Dg(x^{(k)})^{-1}g(x^{(k)})+x^{*}-x^{(k)}+\int_{0}^{1}(1-s)Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})[e^{(k)},e^{(k)}]ds.
\]

\end_inset

Since 
\begin_inset Formula $x^{(k+1)}=x^{(k)}-Dg(x^{(k)})^{-1}g(x^{(k)})$
\end_inset

,
 we have
\begin_inset Formula 
\[
x^{(k+1)}-x^{*}=\int_{0}^{1}(1-s)Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})[e^{(k)},e^{(k)}]ds.
\]

\end_inset

So,
 we have
\begin_inset Formula 
\begin{align*}
\|x^{(k+1)}-x^{*}\| & \leq\int_{0}^{1}(1-s)\|Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})[e^{(k)},e^{(k)}]\|ds\\
 & \leq\int_{0}^{1}(1-s)\|Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})\|_{\op}ds\cdot\|x^{(k)}-x^{*}\|^{2}.
\end{align*}

\end_inset

Since 
\begin_inset Formula $x^{(k)}\rightarrow x^{*}$
\end_inset

,
 we have that 
\begin_inset Formula 
\[
Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})\rightarrow Dg(x^{*})^{-1}D^{2}g(x^{*})
\]

\end_inset

and for large enough 
\begin_inset Formula $k$
\end_inset

,
 we can assume 
\begin_inset Formula 
\[
\|Dg(x^{(k)})^{-1}D^{2}g(x^{(k)}+se^{(k)})\|_{\op}\le2\|Dg(x^{*})^{-1}D^{2}g(x^{*})\|_{\op}.
\]

\end_inset

The result follows.
\end_layout

\begin_layout Standard
This argument above uses only that 
\begin_inset Formula $g\in\mathcal{C}^{2}$
\end_inset

 and does not require convexity.
 Without further global assumptions on 
\begin_inset Formula $g$
\end_inset

,
 the Newton method does not always converge to a root.
 The argument above only shows that if the algorithm converges,
 then it converges quadratically eventually.
 We call this 
\begin_inset Quotes eld
\end_inset

local
\begin_inset Quotes erd
\end_inset

 convergence since it only gives a bound when 
\begin_inset Formula $x^{(k)}$
\end_inset

 is close enough to 
\begin_inset Formula $x^{*}$
\end_inset

.
 In comparison,
 all earlier analyses in this book are about global convergence,
 bounding the total number of iterations.
 In practice,
 both analyses are important;
 global convergence makes sure the algorithm is robust and local quadratic convergence makes sure the algorithm converges to machine accuracy quickly enough.
 The local quadratic convergence is particularly important for simple problems such as computing 
\begin_inset Formula $\sqrt{x}$
\end_inset

.
\end_layout

\begin_layout Subsection*
Finding the largest root of a real-rooted polynomial
\end_layout

\begin_layout Standard
In general,
 the Newton method does not converge globally.
 Here,
 we give an application that does converge 
\begin_inset Quotes eld
\end_inset

globally
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Theorem
Given a polynomial 
\begin_inset Formula $g(x)=\sum_{i=1}^{n}a_{i}x^{i}$
\end_inset

 with only real roots.
 Let 
\begin_inset Formula $\lambda_{1}$
\end_inset

 be its largest root.
 Suppose that 
\begin_inset Formula $\lambda_{1}\leq x^{(0)}$
\end_inset

.
 Then,
 after 
\begin_inset Formula $O(n\log(\frac{1}{\epsilon}))$
\end_inset

 iterations,
 the Newton method finds 
\begin_inset Formula $x^{(k)}$
\end_inset

 such that
\begin_inset Formula 
\[
\lambda_{1}\leq x^{(k)}\leq\lambda_{1}+\varepsilon\cdot(x^{(0)}-\lambda_{1}).
\]

\end_inset


\end_layout

\begin_layout Proof
Since the roots of 
\begin_inset Formula $g$
\end_inset

 are real,
 we can write
\begin_inset Formula 
\[
g(x)=b\cdot\prod_{i=1}^{n}(x-\lambda_{i})
\]

\end_inset

for some 
\begin_inset Formula $b$
\end_inset

 with 
\begin_inset Formula $\lambda_{n}\leq\lambda_{n-1}\leq\cdots\leq\lambda_{1}$
\end_inset

.
 Then,
 we have that 
\begin_inset Formula $g'(x)=b\cdot\sum_{i}\prod_{j\neq i}(x-\lambda_{j})$
\end_inset

 and hence
\begin_inset Formula 
\[
\frac{g(x)}{g'(x)}=\frac{1}{\sum_{i}\frac{1}{x-\lambda_{i}}}.
\]

\end_inset

For any 
\begin_inset Formula $x\geq\lambda_{1}$
\end_inset

,
 we have 
\begin_inset Formula 
\begin{align*}
\frac{g(x)}{g'(x)} & \leq\frac{1}{\frac{1}{x-\lambda_{1}}}=x-\lambda_{1},\\
\frac{g(x)}{g'(x)}\geq & \frac{1}{\sum_{i}\frac{1}{x-\lambda_{1}}}=\frac{x-\lambda_{1}}{n}.\tag{{1}}
\end{align*}

\end_inset

Hence,
 by induction,
 we have that 
\begin_inset Formula $x^{(k)}\geq\lambda_{1}$
\end_inset

 during every step 
\begin_inset Formula $k$
\end_inset

 and that
\begin_inset Formula 
\[
x^{(k+1)}-\lambda_{1}\leq\left(1-\frac{1}{n}\right)(x^{(k)}-\lambda_{1})
\]

\end_inset

where we use 
\begin_inset Formula $(1)$
\end_inset

 to show the inductive step above.
\end_layout

\begin_layout Exercise
Consider the following iteration:
 
\begin_inset Formula 
\[
x^{t+1}=x^{t}-\frac{1}{n^{1/k}}\frac{g^{(k-1)}(x)}{g^{(k)}(x)}
\]

\end_inset

where 
\begin_inset Formula $g^{(k)}(x)=\sum_{i}\frac{1}{(x-\lambda_{i})^{k}}$
\end_inset

.
 For 
\begin_inset Formula $k=1$
\end_inset

,
 this is exactly the Newton iteration as 
\begin_inset Formula $g^{(0)}(x)=n$
\end_inset

.
 Show that when applied to a degree 
\begin_inset Formula $n$
\end_inset

 real-rooted polynomial,
 starting with 
\begin_inset Formula $x^{(0)}>\lambda_{1}$
\end_inset

,
 in each iteration the distance to the largest root decreases by a factor of 
\begin_inset Formula $1-\frac{1}{n^{1/k}}$
\end_inset

.
\end_layout

\begin_layout Standard
Such a dependence of 
\begin_inset Formula $\log\frac{1}{\epsilon}$
\end_inset

 is called linear convergence.
 The convergence of the Newton method can be quadratic,
 when close enough to a root.
\end_layout

\begin_layout Theorem
Assume that 
\begin_inset Formula $|f'(x^{*})|\geq\alpha$
\end_inset

 at 
\begin_inset Formula $f(x^{*})=0$
\end_inset

 and f' is 
\begin_inset Formula $L$
\end_inset

-Lipschitz.
 Then,
 if 
\begin_inset Formula $|x^{0}-x^{*}|\le\frac{\alpha}{2L}$
\end_inset

,
\begin_inset Formula 
\[
|x^{(k)}-x^{*}|\leq\frac{\alpha}{L}\left(\frac{L}{\alpha}|x^{(0)}-x^{*}|\right)^{2^{(k)}}.
\]

\end_inset


\end_layout

\begin_layout Proof
The iteration is 
\begin_inset Formula 
\[
x^{(k+1)}-x^{*}=x^{(k)}-x^{*}-\frac{f(x^{(k)})}{f'(x^{(k)})}.
\]

\end_inset


\begin_inset Formula 
\begin{align*}
f(x^{*}) & =f(x^{(k)})+\int_{x^{k}}^{x^{*}}f'(z)dz=f(x^{(k)})+f'(x^{(k)})(x^{*}-x^{(k)})+\int_{x^{k}}^{x^{*}}(f'(z)-f'(x^{(k)}))dz.
\end{align*}

\end_inset

Therefore,
 
\begin_inset Formula 
\begin{align*}
x^{(k+1)}-x^{*} & =\frac{1}{f'(x^{(k)})}\int_{x^{k}}^{x^{*}}(f'(z)-f'(x^{(k)}))dz.\\
 & \leq\frac{L}{|f'(x^{(k)})|}\int_{x^{k}}^{x^{*}}|z-x^{(k)}|dz\\
 & =\frac{L|x^{*}-x^{(k)}|^{2}}{2|f'(x^{(k)})|}.
\end{align*}

\end_inset

Since 
\begin_inset Formula $f'(x^{(k)})\geq f'(x^{*})-L|x^{(k)}-x^{*}|$
\end_inset


\begin_inset Formula 
\[
\left|x^{(k+1)}-x^{*}\right|\le\frac{L}{2(\alpha-L|x^{(k)}-x^{*}|)}|x^{(k)}-x^{*}|^{2}.
\]

\end_inset

So,
 if 
\begin_inset Formula $|x^{(k)}-x^{*}|\leq\frac{\alpha}{2L}$
\end_inset

,
\begin_inset Formula 
\[
|x^{(k+1)}-x^{*}|\leq\frac{L}{\alpha}|x^{(k)}-x^{*}|^{2}\le\frac{1}{2}|x^{(k)}-x^{*}|
\]

\end_inset

and 
\begin_inset Formula 
\[
\frac{L}{\alpha}|x^{(k+1)}-x^{*}|\leq\left(\frac{L}{\alpha}|x^{(k)}-x^{*}|\right)^{2}.
\]

\end_inset

After 
\begin_inset Formula $t$
\end_inset

 steps,
\begin_inset Formula 
\[
\frac{L}{\alpha}|x^{(k)}-x^{*}|\leq\left(\frac{L}{\alpha}\epsilon_{0}\right)^{2^{(k)}}
\]

\end_inset

implying 
\begin_inset Formula $|x^{(k)}-x^{*}|<\epsilon$
\end_inset

 after 
\begin_inset Formula $\log\log(\frac{L\epsilon_{0}}{\alpha\epsilon})$
\end_inset

 steps.
 Note that 
\begin_inset Formula $f$
\end_inset

 has not been assumed to be convex or polynomial.
\end_layout

\begin_layout Standard
Moving back to optimization,
 we can view the goal as finding a root of 
\begin_inset Formula $\nabla f(x)=0$
\end_inset

.
 Newton's iteration is the update 
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-(\nabla^{2}f(x^{(k)}))^{-1}\nabla f(x^{(k)}).
\]

\end_inset

By the above proof,
 Newton's iteration has quadratic convergence from points close enough to the optimal.
\end_layout

\begin_layout Subsection*
Quasi-Newton Method
\end_layout

\begin_layout Standard
When the Jacobian of 
\begin_inset Formula $g$
\end_inset

 is not available,
 we can approximate it using the function itself.
 For one dimension,
 we can approximate the Newton method and get the following secant method 
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-g(x^{(k)})\frac{x^{(k)}-x^{(k-1)}}{g(x^{(k)})-g(x^{(k-1)})}
\]

\end_inset

where we approximated the 
\begin_inset Formula $g'(x^{(k)})$
\end_inset

 by 
\begin_inset Formula $\frac{g(x^{(k)})-g(x^{(k-1)})}{x^{(k)}-x^{(k-1)}}$
\end_inset

.
 For nice enough functions,
 the convergence rate satisfies 
\begin_inset Formula $\varepsilon_{k+1}\leq C\cdot\varepsilon_{k}^{\frac{1+\sqrt{5}}{2}}$
\end_inset

,
 which is super linearly but not quadratic.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $g$
\end_inset

that is high-dimensional (of dimension at least 
\begin_inset Formula $2$
\end_inset

),
 we need to approximate its Jacobian.
 Let 
\begin_inset Formula $J^{(k)}$
\end_inset

 be the approximate Jacobian we maintained in the 
\begin_inset Formula $k^{th}$
\end_inset

 iteration.
 Similar to the secant method,
 we want to enforce
\begin_inset Formula 
\[
J^{(k+1)}\cdot(x^{(k)}-x^{(k-1)})=g(x^{(k)})-g(x^{(k-1)}).
\]

\end_inset

In dimension higher than one,
 this does not uniquely define the 
\begin_inset Formula $J^{(k+1)}$
\end_inset

.
 One natural choice is to find 
\begin_inset Formula $J^{(k+1)}$
\end_inset

 that is closest to 
\begin_inset Formula $J^{(k)}$
\end_inset

 while satisfying the equation above.
 Solving the problem
\begin_inset Formula 
\begin{equation}
\min_{J^{(k+1)}\cdot(x^{(k)}-x^{(k-1)})=g(x^{(k)})-g(x^{(k-1)})}\|J^{(k+1)}-J^{(k)}\|_{F},\label{eq:broyden_method_opt}
\end{equation}

\end_inset

we have the update rule
\begin_inset Formula 
\begin{equation}
J^{(k+1)}=J^{(k)}+\frac{y^{(k)}-J^{(k)}s^{(k)}}{\|s^{(k)}\|^{2}}s^{(k)\top}\label{eq:broyden_method}
\end{equation}

\end_inset

where 
\begin_inset Formula $s^{(k)}=x^{(k)}-x^{(k-1)}$
\end_inset

 and 
\begin_inset Formula $y^{(k)}=g(x^{(k)})-g(x^{(k-1)})$
\end_inset

.
\end_layout

\begin_layout Exercise
Prove that the equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:broyden_method"
nolink "false"

\end_inset

) is indeed the minimizer of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:broyden_method_opt"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
\end_layout

\begin_layout Standard
When 
\begin_inset Formula $g$
\end_inset

 is given by the gradient of a convex function 
\begin_inset Formula $f$
\end_inset

,
 we know that the Jacobian of 
\begin_inset Formula $g$
\end_inset

 satisfies 
\begin_inset Formula $Dg=\nabla^{2}f(x)\succeq0$
\end_inset

.
 Therefore,
 we should impose some conditions such that 
\begin_inset Formula $J^{(k)}\succeq0$
\end_inset

 for all 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Subsection*
BFGS algorithm
\end_layout

\begin_layout Standard
Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is one of the most popular quasi-Newton methods.
 The algorithm maintains an approximate Hessian 
\begin_inset Formula $J^{(k)}$
\end_inset

 such that
\end_layout

\begin_layout Itemize
\begin_inset Formula $J^{(k+1)}(x^{(k)}-x^{(k-1)})=\nabla f(x^{(k)})-\nabla f(x^{(k-1)})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $J^{(k+1)}$
\end_inset

 is close to 
\begin_inset Formula $J^{(k)}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $J^{(k)}\succ0$
\end_inset

.
\end_layout

\begin_layout Standard
To achieve all of these conditions,
 the natural optimization is 
\begin_inset Formula 
\[
J^{(k+1)}\defeq\arg\min_{Js=y,J=J^{\top}}\norm{W^{\frac{1}{2}}\left(J^{-1}-\left(J^{(k)}\right)^{-1}\right)W^{\frac{1}{2}}}_{F}^{2}
\]

\end_inset

where 
\begin_inset Formula $s^{(k)}=x^{(k)}-x^{(k-1)}$
\end_inset

,
 
\begin_inset Formula $y^{(k)}=\nabla f(x^{(k)})-\nabla f(x^{(k-1)})$
\end_inset

 and 
\begin_inset Formula $W=\int_{0}^{1}\nabla^{2}f(x^{(k-1)}+s(x^{(k)}-x^{(k-1)}))ds$
\end_inset

 (or any 
\begin_inset Formula $W$
\end_inset

 such that 
\begin_inset Formula $Wy=s$
\end_inset

).
 In some sense,
 the 
\begin_inset Formula $W^{-\frac{1}{2}}$
\end_inset

 is just a correct change of variables so that the algorithm is affine invariant.
 Solving the equation above 
\begin_inset CommandInset citation
LatexCommand cite
key "fletcher1987practical"
literal "false"

\end_inset

,
 one obtain the update
\begin_inset Formula 
\begin{equation}
\left(J^{(k+1)}\right)^{-1}=(I-\rho_{k}\cdot s^{(k)}y^{(k)\top})\left(J^{(k)}\right)^{-1}(I-\rho_{k}\cdot y^{(k)}s^{(k)\top})+\rho_{k}\cdot s^{(k)}s^{(k)\top}\label{eq:recursive}
\end{equation}

\end_inset

where 
\begin_inset Formula $\rho_{k}=\frac{1}{y^{(k)\top}s^{(k)}}$
\end_inset

.
 Alternatively,
 one can also show that 
\begin_inset CommandInset citation
LatexCommand cite
key "fletcher1991new"
literal "false"

\end_inset


\begin_inset Formula 
\[
J^{(k+1)}=\arg\min_{Js=y,J=J^{\top}}D_{KL}(\mathcal{N}(0,J)\|\mathcal{N}(0,J^{(k)})).
\]

\end_inset


\end_layout

\begin_layout Standard
To implement the BFGS algorithm,
 it suffices to compute 
\begin_inset Formula $\left(J^{(k)}\right)^{-1}\nabla f(x^{(k)})$
\end_inset

.
 Therefore,
 we can directly use the recursive formula (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:recursive"
nolink "false"

\end_inset

) to compute 
\begin_inset Formula $\left(J^{(k)}\right)^{-1}\nabla f(x^{(k)})$
\end_inset

 instead of maintaining 
\begin_inset Formula $J^{(k)}$
\end_inset

 or 
\begin_inset Formula $\left(J^{(k)}\right)^{-1}$
\end_inset

 explicitly.
\end_layout

\begin_layout Standard
In practice,
 the recursive formula 
\begin_inset Formula $J^{(k)}$
\end_inset

 becomes too expensive and hence one can stop the recursive formula after constant steps,
 which gives the limited-memory BFGS algorithm.
\end_layout

\begin_layout Subsection*
\begin_inset Note Note
status collapsed

\begin_layout Subsection*
Semismooth Newton algorithm
\end_layout

\begin_layout Plain Layout

\series bold
This section is not done yet.
 
\end_layout

\begin_layout Plain Layout
Consider the function 
\begin_inset Formula $f(x)=\begin{cases}
x^{2} & \text{if }x>0\\
1.5x^{2} & \text{else}
\end{cases}$
\end_inset

.
 When we apply Newton method to find its minimizer,
 it is clear that it converges in 
\begin_inset Formula $1$
\end_inset

 step even it is not in 
\begin_inset Formula $\mathcal{C}^{2}$
\end_inset

.
 
\end_layout

\begin_layout Plain Layout
In general,
 Newton method converges superlinearly for a large class of problems.
 For the rest of the section,
 we will go back to the root-finding setting.
\begin_inset Note Note
status open

\begin_layout Plain Layout
reference:
\end_layout

\begin_layout Plain Layout
The semismooth-related properties of a merit function and a descent method for the nonlinear complementarity problem
\end_layout

\begin_layout Plain Layout
A Survey of Some Nonsmooth Equations and Smoothing Newton Methods
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
Suppose 
\begin_inset Formula $g:\R^{n}\rightarrow\R^{n}$
\end_inset

 is locally Lipschitz.
 By Rademacher’s Theorem,
 
\begin_inset Formula $g$
\end_inset

 is almost everywhere differentiable.
 We define
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $D_{g}=\{x|g\text{ is differentiable at }x\}$
\end_inset

.
 Let 
\begin_inset Formula $\partial_{B}g(x)=\{\lim_{x^{j}\rightarrow x,x^{j}\in D_{g}}Dg(x^{j})\}$
\end_inset

.
 Let 
\begin_inset Formula $\partial g(x)=\conv\partial_{B}g(x)$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Definition
We say 
\begin_inset Formula $g$
\end_inset

 is semismooth at 
\begin_inset Formula $x$
\end_inset

 if 
\begin_inset Formula $g$
\end_inset

 is continuous and for any 
\begin_inset Formula $V\in\partial g(x+h)$
\end_inset

,
 
\begin_inset Formula $g(x+h)-g(x)-Vh=o(\|h\|)$
\end_inset

.
\end_layout

\begin_layout Exercise
Prove that 
\begin_inset Formula $\nabla f$
\end_inset

 is semismooth for any convex function 
\begin_inset Formula $f$
\end_inset

.
 Prove that 
\begin_inset Formula $\phi(a,b)=\sqrt{a^{2}+b^{2}}-(a+b)$
\end_inset

 is semismooth.
\end_layout

\begin_layout Plain Layout
The function 
\begin_inset Formula $\phi(a,b)$
\end_inset

 is important because 
\begin_inset Formula $a\geq0,b\geq0,ab=0$
\end_inset

 if and only if 
\begin_inset Formula $\phi(a,b)=0$
\end_inset

.
 Hence,
 this is useful for the nonlinear complementarity problem.
\end_layout

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Theorem
Consider the generalized Newton method 
\begin_inset Formula $x^{(k+1)}\leftarrow x^{(k)}-V_{k}^{-1}g(x^{(k)})$
\end_inset

 where 
\begin_inset Formula $V_{k}\in\partial(x^{(k)})$
\end_inset

.
 Suppose 
\begin_inset Formula $g(x^{*})=0$
\end_inset

 and that all 
\begin_inset Formula $V\in\partial g(x^{*})$
\end_inset

 are invertible.
 Then generalized Newton method is super-linear convergence in a neighborhood of 
\begin_inset Formula $x^{*}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_body
\end_document

#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass optbook
\use_default_options true
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding auto-legacy
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Standard
In this chapter,
 we study techniques that further exploit the geometry of convex functions and associated norms.
 Many of these techniques are effective in practice for large scale problems.
 We recall the following relevant definitions.
\end_layout

\begin_layout Section
Norms and Local Metrics
\end_layout

\begin_layout Standard
Any centrally symmetric convex body 
\begin_inset Formula $K$
\end_inset

 induces the following norm:
 
\begin_inset Formula 
\[
\norm x_{K}=\inf\left\{ t:x\in tK\right\} .
\]

\end_inset

For the usual Euclidean norm,
 the convex body is the Euclidean ball,
 
\begin_inset Formula $B_{2}^{n}$
\end_inset

.
 Similarly,
 for an 
\begin_inset Formula $\ell_{p}$
\end_inset

 norm,
 the convex body is the unit 
\begin_inset Formula $\ell_{p}$
\end_inset

 ball.
 
\end_layout

\begin_layout Standard
It is often useful to consider affine transformations,
 and the (semi)norms they induce,
 e.g.,
 for a PSD matrix 
\begin_inset Formula $A$
\end_inset

 (would be an induced norm in the case when A is PD),
 we can define the associated norm as follows:
\begin_inset Formula 
\[
\norm x_{A}=\sqrt{x^{\top}Ax}.
\]

\end_inset

The convex body (unit ball) of this norm is an ellipsoid centered at zero and defined by the matrix 
\begin_inset Formula $A$
\end_inset

.
 
\end_layout

\begin_layout Standard
As we have encountered previously in this book,
 convex sets and functions have duals.
 We recall the dual (polar) of a convex set 
\begin_inset Formula $K$
\end_inset

:
 
\begin_inset Formula 
\[
K^{*}=\left\{ y:\forall x\in K,\left\langle x,y\right\rangle \le1\right\} .
\]

\end_inset


\end_layout

\begin_layout Standard
The dual norm for 
\begin_inset Formula $\norm ._{K}$
\end_inset

 is 
\begin_inset Formula $\norm ._{K^{*}}$
\end_inset

.
 We can state a generalized Cauchy-Schwarz inequality.
\end_layout

\begin_layout Fact
For 
\begin_inset Formula $x,y\in\R^{n}$
\end_inset

 and any centrally symmetric convex body 
\begin_inset Formula $K$
\end_inset

,
 we have 
\begin_inset Formula $\left\langle x,y\right\rangle \le\norm x_{K}\norm y_{K^{*}}.$
\end_inset


\end_layout

\begin_layout Standard
In this chapter,
 an important idea will be 
\emph on
local
\emph default
 norms,
 i.e.,
 at each point 
\begin_inset Formula $x$
\end_inset

 in the domain,
 there could be a different norm.
 Indeed,
 in a Riemannian metric 
\begin_inset Formula ${\cal M}$
\end_inset

,
 for every 
\begin_inset Formula $x\in{\cal M},$
\end_inset

 there is a matrix 
\begin_inset Formula $A(x)$
\end_inset

 s.t.
 the norm at 
\begin_inset Formula $x$
\end_inset

 is defined as 
\begin_inset Formula $\norm v_{x}=\sqrt{v^{\top}A(x)v}.$
\end_inset

 A special class of Riemannian metrics of particular interest for us will be 
\emph on
Hessian 
\emph default
metrics (corresponding to Hessian manifolds).
 Here the matrix defining the local norm is the Hessian of a convex,
 twice-differentiable function,
 i.e.,
 
\begin_inset Formula 
\[
A(x)=\nabla^{2}\phi(x)
\]

\end_inset

for some convex function 
\begin_inset Formula $\phi$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Instead of calling it subgradient method.
 Maybe I just say 
\begin_inset Formula $\ell_{2}$
\end_inset

 version and general version?
\end_layout

\end_inset


\end_layout

\begin_layout Section
Mirror Descent
\begin_inset CommandInset label
LatexCommand label
name "sec:mirror_descent"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Maybe adding relative smoothness here?
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
I should mention some geometry is bad..i.e.
 no strongly convex map...
\end_layout

\begin_layout Plain Layout
Also,
 optimal mirror map is given by some martingale
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The cutting plane method is well-suited for minimizing non-smooth convex functions with high accuracy.
 However,
 its relatively large polynomial time complexity and quadratic space requirement are not favorable for large scale problems.
 Here we discuss a different approach to minimize a non-smooth function with low accuracy.
\end_layout

\begin_layout Subsection
Subgradient method
\end_layout

\begin_layout Standard
In this section,
 we consider the constrained non-smooth minimization problem 
\begin_inset Formula $\min_{x\in\mathcal{D}}f(x)$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Mention what difficulty to minimize a non-smooth function and explain the change made to deal with the non-smooth settings.
 highlight that the subgradient is not guaranteed to converge
\end_layout

\end_inset

Recall how to define gradient for non-smooth functions:
\end_layout

\begin_layout Definition
For any convex function 
\begin_inset Formula $f$
\end_inset

,
 we define 
\begin_inset Formula $\partial f(x)$
\end_inset

 be the set of vectors 
\begin_inset Formula $g$
\end_inset

 such that
\begin_inset Formula 
\[
f(y)\geq f(x)+g^{\top}(y-x)\text{ for all }y\in\Rn.
\]

\end_inset

Such a vector 
\begin_inset Formula $g$
\end_inset

 is called a 
\emph on
subgradient
\emph default
 of 
\begin_inset Formula $f$
\end_inset

 at 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithm2e}[H]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
caption{
\end_layout

\end_inset


\begin_inset Formula $\mathtt{SubgradientMethod}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
SetAlgoLined
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Input:

\series default
 Initial point 
\begin_inset Formula $x^{(0)}\in\Rn$
\end_inset

,
 step size 
\begin_inset Formula $h>0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
For{
\end_layout

\end_inset


\begin_inset Formula $k=0,1,\cdots,T-2$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}{
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Pick any 
\begin_inset Formula $g^{(k)}\in\partial f(x^{(k)})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $y^{(k+1)}\leftarrow x^{(k)}-h\cdot g^{(k)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $x^{(k+1)}\leftarrow\pi_{\mathcal{D}}(y^{(k+1)})$
\end_inset

 where 
\begin_inset Formula $\pi_{\mathcal{D}}(y)=\arg\min_{x\in\mathcal{D}}\|x-y\|_{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Return
\end_layout

\end_inset

 
\begin_inset Formula $\frac{1}{T}\sum_{k=0}^{T-1}x^{(k)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithm2e}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that we return the average of all iterates,
 rather than the last iterate,
 as the average is better behaved in the worst case.
 To analyze the algorithm,
 we first need the following Pythagorean theorem.
 This shows that when we project a point to a convex set,
 we get closer to each point in the convex set,
 and how much closer depends on how much we move.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:Pythagorean"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pythagorean Theorem
\end_layout

\end_inset

Given a convex set 
\begin_inset Formula $\mathcal{D}$
\end_inset

 and a point 
\begin_inset Formula $y$
\end_inset

,
 let 
\begin_inset Formula $\pi(y)=arg\min_{x\in\mathcal{D}}\|x-y\|_{2}$
\end_inset

.
 For any 
\begin_inset Formula $z\in\mathcal{D}$
\end_inset

,
 we have that
\begin_inset Note Note
status open

\begin_layout Plain Layout
Make a figure for this.
\end_layout

\end_inset


\begin_inset Formula 
\[
\|z-\pi(y)\|_{2}^{2}+\|\pi(y)-y\|_{2}^{2}\leq\|z-y\|_{2}^{2}.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $h(t)=\|(\pi(y)+t(z-\pi(y)))-y\|^{2}$
\end_inset

.
 Since 
\begin_inset Formula $\pi(y)$
\end_inset

 is the closest point to 
\begin_inset Formula $y$
\end_inset

,
 
\begin_inset Formula $h(t)$
\end_inset

 must be minimized at 
\begin_inset Formula $t=0$
\end_inset

.
 So,
 we have that 
\begin_inset Formula $h'(0)\geq0$
\end_inset

.
 i.e.,
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
h'(0)=(\pi(y)-y)^{\top}(z-\pi(y))\geq0.
\]

\end_inset


\end_layout

\begin_layout Proof
(If 
\begin_inset Formula $y$
\end_inset

 is in the set,
 then the statement is trivial since 
\begin_inset Formula $\pi(y)=y$
\end_inset

.
 Otherwise,
 since 
\begin_inset Formula $\pi(y)$
\end_inset

 is the closest point in 
\begin_inset Formula ${\cal D}$
\end_inset

 to 
\begin_inset Formula $y$
\end_inset

,
 the hyperplane normal to 
\begin_inset Formula $\pi(y)-y$
\end_inset

 through 
\begin_inset Formula $\pi(y)$
\end_inset

 separates 
\begin_inset Formula $y$
\end_inset

 from 
\begin_inset Formula $z$
\end_inset

.).
 Hence,
\begin_inset Formula 
\begin{align*}
\|z-y\|_{2}^{2} & =\|z-\pi(y)+\pi(y)-y\|_{2}^{2}\\
 & =\|z-\pi(y)\|_{2}^{2}+2(z-\pi(y))^{\top}(\pi(y)-y)+\|\pi(y)-y\|_{2}^{2}\\
 & \geq\|z-\pi(y)\|_{2}^{2}+\|\pi(y)-y\|_{2}^{2}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now,
 we are ready to analyze the subgradient method.
 It basically involves tracking the squared distance to the optimum,
 
\begin_inset Formula $\|x^{(k+1)}-x^{*}\|_{2}^{2}$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:projected_gradient_descent"

\end_inset

Let 
\begin_inset Formula $f$
\end_inset

 be a convex function that is 
\begin_inset Formula $G$
\end_inset

-Lipschitz in 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm.
 After 
\begin_inset Formula $T$
\end_inset

 steps,
 the subgradient method outputs a point 
\begin_inset Formula $x$
\end_inset

 such that
\begin_inset Formula 
\[
f(x)\leq f(x^{*})+\frac{\|x^{(0)}-x^{*}\|_{2}^{2}}{2hT}+\frac{h}{2}G^{2}
\]

\end_inset

where 
\begin_inset Formula $x^{*}$
\end_inset

 is any point that minimizes 
\begin_inset Formula $f$
\end_inset

 over 
\begin_inset Formula ${\cal D}$
\end_inset

.
\end_layout

\begin_layout Remark*
If the distance 
\begin_inset Formula $\|x^{(0)}-x^{*}\|$
\end_inset

 and the Lipschitz constant 
\begin_inset Formula $G$
\end_inset

 are known,
 we can pick 
\begin_inset Formula $h=\frac{\|x^{(0)}-x^{*}\|_{2}}{G\sqrt{T}}$
\end_inset

 and get
\begin_inset Formula 
\[
f(x)\leq f(x^{*})+\frac{G\cdot\|x^{(0)}-x^{*}\|_{2}}{\sqrt{T}}.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $x^{*}$
\end_inset

 be any point that minimizes 
\begin_inset Formula $f$
\end_inset

.
 Then,
 we have
\begin_inset Formula 
\begin{align*}
\|x^{(k+1)}-x^{*}\|_{2}^{2} & =\|\pi_{\mathcal{D}}(y^{(k+1)})-x^{*}\|_{2}^{2}\\
 & \leq\|y^{(k+1)}-x^{*}\|_{2}^{2}\\
 & =\|x^{(k)}-hg^{(k)}-x^{*}\|_{2}^{2}\\
 & =\|x^{(k)}-x^{*}\|_{2}^{2}-2h\left\langle g^{(k)},x^{(k)}-x^{*}\right\rangle +h^{2}\|g^{(k)}\|_{2}^{2}.
\end{align*}

\end_inset

where we used Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:Pythagorean"
nolink "false"

\end_inset

 in the inequality.
 Since 
\begin_inset Formula $x^{*}$
\end_inset

 lies on the 
\begin_inset Formula $-g^{(k)}$
\end_inset

 direction,
 we expect 
\begin_inset Formula $x^{(k+1)}$
\end_inset

 is closer to 
\begin_inset Formula $x^{*}$
\end_inset

 than 
\begin_inset Formula $x^{(k)}$
\end_inset

 if the step size is small enough (or if we ignore the second order term 
\begin_inset Formula $\|g^{(k)}\|_{2}^{2}$
\end_inset

).
 To bound the distance improvement,
 we apply the definition of subgradient and get
\begin_inset Formula 
\[
f(x^{*})\geq f(x^{(k)})+\left\langle g^{(k)},x^{*}-x^{(k)}\right\rangle .
\]

\end_inset

Therefore,
 we have that
\begin_inset Formula 
\begin{align*}
\|x^{(k+1)}-x^{*}\|_{2}^{2} & \leq\|x^{(k)}-x^{*}\|_{2}^{2}-2h\cdot(f(x^{(k)})-f(x^{*}))+h^{2}\|g^{(k)}\|_{2}^{2}\\
 & \leq\|x^{(k)}-x^{*}\|_{2}^{2}-2h\cdot(f(x^{(k)})-f(x^{*}))+h^{2}G^{2}.
\end{align*}

\end_inset

Note that this equation shows that if the error 
\begin_inset Formula $f(x^{(k)})-f(x^{*})$
\end_inset

 is larger,
 then we move faster towards the optimum.
 Rearranging the terms,
 we have
\begin_inset Formula 
\[
f(x^{(k)})-f(x^{*})\leq\frac{1}{2h}\left(\|x^{(k)}-x^{*}\|_{2}^{2}-\|x^{(k+1)}-x^{*}\|_{2}^{2}\right)+\frac{h}{2}G^{2}.
\]

\end_inset

We sum over all iterations,
 to get
\begin_inset Formula 
\begin{align*}
\frac{1}{T}\sum_{k=0}^{T-1}(f(x^{(k)})-f(x^{*})) & \leq\frac{1}{T}\cdot\frac{1}{2h}(\|x^{(0)}-x^{*}\|_{2}^{2}-\|x^{(T)}-x^{*}\|_{2}^{2})+\frac{h}{2}G^{2}\\
 & \leq\frac{\|x^{(0)}-x^{*}\|_{2}^{2}}{2hT}+\frac{h}{2}G^{2}.
\end{align*}

\end_inset

The result follows from observing that for a convex function,
 
\begin_inset Formula 
\[
f\left(\frac{1}{T}\sum_{k=0}^{T-1}x^{(k)}\right)-f(x^{*})\leq\frac{1}{T}\sum_{k=0}^{T-1}(f(x^{(k)})-f(x^{*})).
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
https://arxiv.org/pdf/1904.12443.pdf
\end_layout

\begin_layout Plain Layout
Seems this is important.
 Mention last point.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Intuition of Mirror Descent
\end_layout

\begin_layout Standard
Consider using the subgradient method above to minimize 
\begin_inset Formula $f(x)=\sum_{i}|x_{i}|$
\end_inset

 over the unit ball 
\begin_inset Formula $B(0,1)$
\end_inset

.
 The subgradient of 
\begin_inset Formula $f$
\end_inset

 is given by 
\begin_inset Formula $\text{sign}(x)$
\end_inset

 (assuming 
\begin_inset Formula $x_{i}\neq0$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

).
 Therefore,
 we have that the Lipschitz constant is bounded by the Euclidean norm of a vector with 
\begin_inset Formula $\pm1$
\end_inset

 entries,
 i.e.,
 
\begin_inset Formula $G=\sqrt{n}$
\end_inset

 and the set is contained in a ball of radius 
\begin_inset Formula $R=1$
\end_inset

.
 Hence,
 by the main theorem of the previous section,
 the error is bounded by 
\begin_inset Formula $\sqrt{\frac{n}{T}}$
\end_inset

 after 
\begin_inset Formula $T$
\end_inset

 steps,
 which grows with the dimension.
 Intuitively,
 the dimension dependence comes from the fact that we must take a tiny step size to avoid changing the variables too much.
 For example,
 if we take a constant step size 
\begin_inset Formula $h$
\end_inset

 and start at the point 
\begin_inset Formula $x=(1,\frac{1}{n},\frac{1}{n},\cdots,\frac{1}{n})$
\end_inset

,
 then we will get a point with 
\begin_inset Formula $x_{i}$
\end_inset

 constant in all directions 
\begin_inset Formula $i\neq1$
\end_inset

,
 and this increases the function 
\begin_inset Formula $f$
\end_inset

 dramatically from 
\begin_inset Formula $\Theta(1)$
\end_inset

 to 
\begin_inset Formula $\Theta(nh)$
\end_inset

.
 Therefore,
 to get constant error,
 we need to take step size 
\begin_inset Formula $h\le\frac{1}{n}$
\end_inset

 and hence we need 
\begin_inset Formula $\Omega(n)$
\end_inset

 iterations.
\end_layout

\begin_layout Standard
Conceptually,
 the step 
\begin_inset Formula $x=x-\eta g$
\end_inset

 does not make sense either.
 Imagine the problem is infinitely dimensional.
 Note that 
\begin_inset Formula $f(x)<+\infty$
\end_inset

 for any 
\begin_inset Formula 
\[
x\in\ell_{1}=\{x\in\R^{\mathbb{N}}:\sum_{i}|x_{i}|<\infty\}.
\]

\end_inset

On the other hand,
 its gradient 
\begin_inset Formula $g$
\end_inset

 lives in 
\begin_inset Formula $\ell_{\infty}$
\end_inset

 space;
 the dual space of 
\begin_inset Formula $\ell_{1}$
\end_inset

 is 
\begin_inset Formula $\ell_{\infty}$
\end_inset

 (in general,
 
\begin_inset Formula $\ell_{p}$
\end_inset

 is dual to 
\begin_inset Formula $\ell_{q}$
\end_inset

 where 
\begin_inset Formula $(1/p)+(1/q)=1$
\end_inset

).
 Since 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

 are not in the same space,
 the term 
\begin_inset Formula $x-\eta g$
\end_inset

 does not make sense.
 More precisely,
 we have the following tautology (directly follows from the definition of dual space,
 namely the set of all linear maps in the original space).
\end_layout

\begin_layout Definition
A 
\emph on
Banach space 
\emph default
over the reals is a vector space over the reals together with a norm that defines a complete metric space,
 i.e.,
 for any Cauchy sequence,
 
\begin_inset Formula $X=(x_{i})_{i=1}^{\infty}$
\end_inset

,
 there is a vector 
\begin_inset Formula $x$
\end_inset

 s.t.
 
\begin_inset Formula $\lim_{n\rightarrow\infty}\norm{x_{n}-x}=0$
\end_inset

.
 
\end_layout

\begin_layout Fact
\begin_inset CommandInset label
LatexCommand label
name "lem:grad_dual"

\end_inset

Given any Banach space 
\begin_inset Formula $\mathcal{D}$
\end_inset

 over the reals and a continuously differentiable function 
\begin_inset Formula $f$
\end_inset

 from 
\begin_inset Formula $\mathcal{D}$
\end_inset

 to 
\begin_inset Formula $\R$
\end_inset

,
 its gradient 
\begin_inset Formula $\nabla f(x)\in\mathcal{D}^{*}$
\end_inset

 for any 
\begin_inset Formula $x$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Need to define Banach space somewhere...
 prelims?
\end_layout

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Explain the dual space better.
 Maybe proving this.
 Not using Banach space.
\end_layout

\begin_layout Plain Layout
Maybe just say 
\begin_inset Formula $f:\ell_{1}\rightarrow R$
\end_inset

 that is 
\begin_inset Formula $\ell_{1}$
\end_inset

-Lipschitz.
 then 
\begin_inset Formula $\|\nabla f\|_{\infty}\leq1$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In general,
 if the function 
\begin_inset Formula $f$
\end_inset

 is on the primal space 
\begin_inset Formula $\mathcal{D}$
\end_inset

,
 then the gradient 
\begin_inset Formula $g$
\end_inset

 lives in the dual space 
\begin_inset Formula $\mathcal{D}^{*}$
\end_inset

.
 Therefore,
 we need to map 
\begin_inset Formula $x$
\end_inset

 from the primal space 
\begin_inset Formula $\mathcal{D}$
\end_inset

 to the dual space 
\begin_inset Formula $\mathcal{D}^{*}$
\end_inset

,
 update its position,
 then map the point back to the original space 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
\end_layout

\begin_layout Standard
In fact,
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:grad_dual"
nolink "false"

\end_inset

 gives us one such map,
 
\begin_inset Formula $\nabla f$
\end_inset

.
 Consider the following algorithm:
 Starting at 
\begin_inset Formula $x$
\end_inset

.
 We use 
\begin_inset Formula $\nabla f(x)$
\end_inset

 to map 
\begin_inset Formula $x$
\end_inset

 to the dual space 
\begin_inset Formula $y=\nabla f(x)$
\end_inset

.
 Then,
 we apply the gradient step on the dual 
\begin_inset Formula $y^{\new}=y-\nabla f(x)$
\end_inset

 and map it back to the primal space,
 namely finding 
\begin_inset Formula $x^{\new}$
\end_inset

 such that 
\begin_inset Formula $\nabla f(x^{\new})=y^{\new}$
\end_inset

.
 Note that 
\begin_inset Formula $y^{\new}=0$
\end_inset

 and hence 
\begin_inset Formula $x^{\new}$
\end_inset

 is exactly a minimizer of 
\begin_inset Formula $f$
\end_inset

.
 So,
 if we can map it back,
 this algorithm solves the problem in one step.
 Unfortunately,
 the task of mapping it back is exactly our original problem.
\end_layout

\begin_layout Standard
Instead of using the same 
\begin_inset Formula $f$
\end_inset

,
 mirror descent uses some other convex function 
\begin_inset Formula $\Phi$
\end_inset

,
 called the 
\emph on
mirror map
\emph default
.
 For constrained problems,
 the mirror map may not bring the point back to a point in 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 Naively,
 one may consider the algorithm 
\begin_inset Formula 
\begin{align*}
\nabla\Phi(y^{(t+1)}) & =\nabla\Phi(x^{(t)})-h\cdot\nabla f(x^{(t)}),\\
x^{(t+1)} & =\arg\min_{x\in\mathcal{D}}\|x-y^{(t+1)}\|_{2}.
\end{align*}

\end_inset

Note that the first step of finding 
\begin_inset Formula $y^{(t+1)}$
\end_inset

 involves solving an optimization problem.
 We will show how to do this optimization later (see 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mirror_descent"
nolink "false"

\end_inset

) but with a proper formulation of the algorithm which takes into account the distance as measured by the mirror map 
\begin_inset Formula $\Phi$
\end_inset

.
\end_layout

\begin_layout Definition
For any strictly convex function 
\begin_inset Formula $\Phi$
\end_inset

,
 we define the Bregman divergence as
\begin_inset Formula 
\[
D_{\Phi}(y,x)=\Phi(y)-\Phi(x)-\left\langle \nabla\Phi(x),y-x\right\rangle .
\]

\end_inset


\end_layout

\begin_layout Definition
Note that 
\begin_inset Formula $D_{\Phi}(y,x)$
\end_inset

 is the error of the first order Taylor expansion of 
\begin_inset Formula $\Phi$
\end_inset

 at 
\begin_inset Formula $x$
\end_inset

.
 Due to the convexity of 
\begin_inset Formula $\Phi$
\end_inset

,
 we have that 
\begin_inset Formula $D_{\Phi}(y,x)\geq0$
\end_inset

.
 Also,
 we note that 
\begin_inset Formula $D_{\Phi}(y,x)$
\end_inset

 is convex in 
\begin_inset Formula $y$
\end_inset

,
 but not necessarily in 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Example
\begin_inset Formula $D_{\Phi}(y,x)=\|y-x\|^{2}$
\end_inset

 for 
\begin_inset Formula $\Phi(x)=\|x\|^{2}$
\end_inset

.
 
\begin_inset Formula $D_{\Phi}(y,x)=\sum_{i}y_{i}\log\frac{y_{i}}{x_{i}}-\sum y_{i}+\sum x_{i}$
\end_inset

 for 
\begin_inset Formula $\Phi(x)=\sum x_{i}\log x_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithm2e}[H]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
caption{
\end_layout

\end_inset


\begin_inset Formula $\mathtt{MirrorDescent}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
SetAlgoLined
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Input:

\series default
 Initial point 
\begin_inset Formula $x^{(0)}=\arg\min_{x\in\mathcal{D}}\Phi(x)$
\end_inset

,
 step size 
\begin_inset Formula $h>0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
For{
\end_layout

\end_inset


\begin_inset Formula $k=0,1,\cdots,T-2$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}{
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Pick any 
\begin_inset Formula $g^{(k)}\in\partial f(x^{(k)})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
tcp{As shown in 
\end_layout

\end_inset

(
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mirror_descent"
nolink "false"

\end_inset

)
\begin_inset ERT
status open

\begin_layout Plain Layout

,
 the next 2 steps can be implemented by an optimization over 
\end_layout

\end_inset


\begin_inset Formula $D_{\Phi}$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

.}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Find 
\begin_inset Formula $y^{(k+1)}$
\end_inset

 such that 
\begin_inset Formula $\nabla\Phi(y^{(k+1)})=\nabla\Phi(x^{(k)})-h\cdot g^{(k)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $x^{(k+1)}\in\pi_{\mathcal{D}}^{\Phi}(y^{(k+1)})$
\end_inset

 where 
\begin_inset Formula $\pi_{\mathcal{D}}^{\Phi}(y)=\arg\min_{x\in\mathcal{D}}D_{\Phi}(x,y)$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Note that one way to think about the problem is that the constraint is defined inside the mirror map.
\end_layout

\begin_layout Plain Layout
Then,
 we don't need to talk about the 
\begin_inset Formula $\mathcal{D}$
\end_inset

 at all.
\end_layout

\begin_layout Plain Layout
There is no 
\begin_inset Formula $y^{(k+1)}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Return
\end_layout

\end_inset

 
\begin_inset Formula $\frac{1}{T}\sum_{k=0}^{T-1}x^{(k)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithm2e}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Mirror Descent step can be written as follows.
 In the second step below,
 we use the fact that the optimization is only over 
\begin_inset Formula $x$
\end_inset

 (and hence all terms in 
\begin_inset Formula $y$
\end_inset

 can be ignored):
\begin_inset Formula 
\begin{align}
x^{(k+1)} & =\arg\min_{x\in\mathcal{D}}D_{\Phi}(x,y^{(k+1)})\nonumber \\
 & =\arg\min_{x\in\mathcal{D}}\Phi(x)-\Phi(y^{(k+1)})-\nabla\Phi(y^{(k+1)})^{\top}(x-y^{(k+1)})\nonumber \\
 & =\arg\min_{x\in\mathcal{D}}\Phi(x)-\nabla\Phi(y^{(k+1)})^{\top}x\nonumber \\
 & =\arg\min_{x\in\mathcal{D}}\Phi(x)-\nabla\Phi(x^{(k)})^{\top}x+hg^{(k)\top}x\nonumber \\
 & =\arg\min_{x\in\mathcal{D}}hg^{(k)\top}x+D_{\Phi}(x,x^{(k)}).\label{eq:mirror_descent}
\end{align}

\end_inset

Note that this is a natural generalization of 
\begin_inset Formula $x^{(k+1)}=\arg\min_{x\in\mathcal{D}}hg^{(k)\top}x+\|x-x^{(k)}\|^{2}.$
\end_inset


\end_layout

\begin_layout Subsection
Analysis of Mirror Descent
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:Pythagorean2"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pythagorean Theorem
\end_layout

\end_inset

Given a convex set 
\begin_inset Formula $\mathcal{D}$
\end_inset

 and a point 
\begin_inset Formula $y$
\end_inset

,
 let 
\begin_inset Formula $\pi(y)=\arg\min D_{\Phi}(x,y)$
\end_inset

.
 For any 
\begin_inset Formula $z\in\mathcal{D}$
\end_inset

,
 we have that
\begin_inset Formula 
\[
D_{\Phi}(z,\pi(y))+D_{\Phi}(\pi(y),y)\leq D_{\Phi}(z,y).
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $h(t)=D_{\Phi}(\pi(y)+t(z-\pi(y)),y)$
\end_inset

.
 Since 
\begin_inset Formula $h(t)$
\end_inset

 is minimized at 
\begin_inset Formula $t=0$
\end_inset

,
 we have that 
\begin_inset Formula $h'(0)\geq0$
\end_inset

.
 Hence,
 we have
\begin_inset Formula 
\[
h'(0)=(\nabla\Phi(\pi(y))-\nabla\Phi(y))^{\top}(z-\pi(y))\geq0.
\]

\end_inset

Hence,
\begin_inset Formula 
\begin{align*}
D_{\Phi}(z,y) & =D_{\Phi}(z,\pi(y))+2(z-\pi(y))^{\top}(\nabla\Phi(\pi(y))-\nabla\Phi(y))+D_{\Phi}(\pi(y),y)\\
 & \geq D_{\Phi}(z,\pi(y))+D_{\Phi}(\pi(y),y).
\end{align*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:mirror_descent"

\end_inset

Let 
\begin_inset Formula $f$
\end_inset

 be a 
\begin_inset Formula $G$
\end_inset

-Lipschitz convex function on 
\begin_inset Formula ${\cal D}$
\end_inset

 with respect to some norm 
\begin_inset Formula $\|\cdot\|$
\end_inset

.
 Let 
\begin_inset Formula $\Phi$
\end_inset

 be a 
\begin_inset Formula $\rho$
\end_inset

-strongly convex function on 
\begin_inset Formula $\mathcal{D}$
\end_inset

 with respect to 
\begin_inset Formula $\|\cdot\|$
\end_inset

 with squared diameter 
\begin_inset Formula $R^{2}=\sup_{x\in\mathcal{D}}\Phi(x)-\Phi(x^{(0)})$
\end_inset

.
 Then,
 mirror descent outputs 
\begin_inset Formula $x$
\end_inset

 such that
\begin_inset Formula 
\[
f(x)-\min_{x}f(x)\leq\frac{R^{2}}{hT}+\frac{h}{2\rho}G^{2}.
\]

\end_inset


\end_layout

\begin_layout Remark
We say a function 
\begin_inset Formula $f$
\end_inset

 is 
\begin_inset Formula $\rho$
\end_inset

 strongly convex with respect to the norm 
\begin_inset Formula $\|\cdot\|$
\end_inset

 if for any 
\begin_inset Formula $x,y$
\end_inset

,
 we have 
\begin_inset Formula 
\[
f(y)\geq f(x)+\left\langle \nabla f(x),y-x\right\rangle +\frac{\rho}{2}\|y-x\|^{2}.
\]

\end_inset

The usual strong convexity is with respect to the Euclidean norm.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
Picking 
\begin_inset Formula $h=\frac{R}{G}\sqrt{\frac{2\rho}{T}}$
\end_inset

,
 we get the rate 
\begin_inset Formula $f(x)\leq\min_{x}f(x)+GR\sqrt{\frac{2}{\rho T}}.$
\end_inset


\end_layout

\begin_layout Proof
This proof is a complete 
\begin_inset Quotes eld
\end_inset

mirror
\begin_inset Quotes erd
\end_inset

 of the proof in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:projected_gradient_descent"
nolink "false"

\end_inset

.
\begin_inset Formula 
\begin{align*}
D_{\Phi}(x^{*},x^{(k+1)}) & \leq D_{\Phi}(x^{*},y^{(k+1)})-D_{\Phi}(x^{(k+1)},y^{(k+1)})\\
 & =D_{\Phi}(x^{*},x^{(k)})+(x^{*}-x^{(k)})^{\top}(\nabla\Phi(x^{(k)})-\nabla\Phi(y^{(k+1)}))+D_{\Phi}(x^{(k)},y^{(k+1)})-D_{\Phi}(x^{(k+1)},y^{(k+1)})\\
 & =D_{\Phi}(x^{*},x^{(k)})-h\cdot\left\langle g^{(k)},x^{(k)}-x^{*}\right\rangle +D_{\Phi}(x^{(k)},y^{(k+1)})-D_{\Phi}(x^{(k+1)},y^{(k+1)})
\end{align*}

\end_inset

where we used Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:Pythagorean2"
nolink "false"

\end_inset

 in the inequality.
 Using the definition of subgradient,
 we have that
\begin_inset Formula 
\[
f(x^{*})\geq f(x^{(k)})+\left\langle g^{(k)},x^{*}-x^{(k)}\right\rangle .
\]

\end_inset

Therefore,
 we have that
\begin_inset Formula 
\begin{align*}
D_{\Phi}(x^{*},x^{(k+1)}) & \leq D_{\Phi}(x^{*},x^{(k)})-h\cdot(f(x^{(k)})-f(x^{*}))+D_{\Phi}(x^{(k)},y^{(k+1)})-D_{\Phi}(x^{(k+1)},y^{(k+1)}).
\end{align*}

\end_inset

Next we note that
\begin_inset Formula 
\begin{align*}
 & D_{\Phi}(x^{(k)},y^{(k+1)})-D_{\Phi}(x^{(k+1)},y^{(k+1)})\\
= & \Phi(x^{(k)})-\Phi(x^{(k+1)})-\nabla\Phi(y^{(k+1)})^{\top}(x^{(k)}-x^{(k+1)})\\
\leq & \left\langle \nabla\Phi(x^{(k)})-\nabla\Phi(y^{(k+1)}),x^{(k)}-x^{(k+1)}\right\rangle -\frac{\rho}{2}\|x^{(k)}-x^{(k+1)}\|^{2}\\
= & h\cdot\left\langle g^{(k)},x^{(k)}-x^{(k+1)}\right\rangle -\frac{\rho}{2}\|x^{(k)}-x^{(k+1)}\|^{2}\\
\leq & hG\|x^{(k)}-x^{(k+1)}\|-\frac{\rho}{2}\|x^{(k)}-x^{(k+1)}\|^{2}\\
\leq & \frac{(hG)^{2}}{2\rho}.
\end{align*}

\end_inset

Hence,
 we have
\begin_inset Formula 
\begin{align*}
D_{\Phi}(x^{*},x^{(k+1)}) & \leq D_{\Phi}(x^{*},x^{(k)})-h\cdot(f(x^{(k)})-f(x^{*}))+\frac{(hG)^{2}}{2\rho}.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Rearranging the terms,
 we have
\begin_inset Formula 
\[
f(x^{(k)})-f(x^{*})\leq\frac{1}{h}\left(D_{\Phi}(x^{*},x^{(k)})-D_{\Phi}(x^{*},x^{(k+1)})\right)+\frac{hG^{2}}{2\rho}.
\]

\end_inset

Summing over all iterations,
 we have
\begin_inset Formula 
\begin{align*}
\frac{1}{T}\sum_{k=0}^{T-1}(f(x^{(k)})-f(x^{*})) & \leq\frac{1}{T}\cdot\frac{1}{h}(D_{\Phi}(x^{*},x^{(0)})-D_{\Phi}(x^{*},x^{(T)}))+\frac{h}{2\rho}G^{2}\\
 & \leq\frac{1}{hT}D_{\Phi}(x^{*},x^{(0)})+\frac{h}{2\rho}G^{2}\\
 & \leq\frac{R^{2}}{hT}+\frac{h}{2\rho}G^{2}.
\end{align*}

\end_inset

Using the fact that 
\begin_inset Formula $x^{(0)}$
\end_inset

 was chosen as a minimum of 
\begin_inset Formula $\Phi(x)$
\end_inset

,
 
\begin_inset Formula 
\begin{align*}
D_{\Phi}(x^{*},x^{(0)})= & (\Phi(x^{*})-\Phi(x^{(0)})-\left\langle \nabla\Phi(x^{(0)}),x^{*}-x^{(0)}\right\rangle )\\
= & \Phi(x^{*})-\Phi(x^{(0)})\\
\le & R^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
The result follows from the convexity of 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Subsection
Multiplicative Weight Update
\end_layout

\begin_layout Standard
In this section,
 we discuss mirror descent under the map 
\begin_inset Formula $\Phi(x)=\sum x_{i}\log x_{i}$
\end_inset

 with the convex set being the simplex 
\begin_inset Formula $\mathcal{D}=\{x_{i}\geq0,\sum x_{i}=1\}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Step Formula
\end_layout

\begin_layout Standard
As we showed in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mirror_descent"
nolink "false"

\end_inset

),
 we have that
\begin_inset Formula 
\[
x^{(k+1)}=\arg\min_{x\in\mathcal{D}}hg^{(k)\top}x+D_{\Phi}(x,x^{(k)}).
\]

\end_inset

Note that
\begin_inset Formula 
\begin{align*}
D_{\Phi}(x,x^{(k)}) & =\sum x_{i}\log x_{i}-\sum x_{i}^{(k)}\log x_{i}^{(k)}-\sum_{i}(1+\log x_{i}^{(k)})(x_{i}-x_{i}^{(k)})\\
 & =\sum x_{i}\log\frac{x_{i}}{x_{i}^{(k)}}
\end{align*}

\end_inset

where we used that 
\begin_inset Formula $\sum_{i}x_{i}=\sum_{i}x_{i}^{(k)}$
\end_inset

.
 Hence,
 the step is simply 
\begin_inset Formula 
\[
x^{(k+1)}=\arg\min_{\sum x_{i}=1,x_{i}\geq0}hg^{(k)\top}x+\sum x_{i}\log\frac{x_{i}}{x_{i}^{(k)}}.
\]

\end_inset

Note that the optimality condition is given by
\begin_inset Note Note
status open

\begin_layout Plain Layout
need to add some basics on Lagrange multipliers.
\end_layout

\end_inset


\begin_inset Formula 
\[
hg_{i}^{(k)}+\log\frac{x_{i}^{(k+1)}}{x_{i}^{(k)}}+1-\lambda=0
\]

\end_inset

for some Lagrangian multiplier 
\begin_inset Formula $\lambda$
\end_inset

.
 Rewriting,
 we have
\begin_inset Formula 
\[
x_{i}^{(k+1)}=e^{-hg_{i}^{(k)}}x_{i}^{(k)}/Z
\]

\end_inset

for some normalization constant 
\begin_inset Formula $Z$
\end_inset

.
 Note that this algorithm multiplies the current 
\begin_inset Formula $x$
\end_inset

 with a multiplicative factor and hence it is also called multiplicative weight update.
\end_layout

\begin_layout Paragraph
Strong Convexity
\end_layout

\begin_layout Standard
To bound the strong convexity parameter,
 we note that
\begin_inset Formula 
\[
\Phi(y)-\Phi(x)-\left\langle \nabla\Phi(x),y-x\right\rangle =\frac{1}{2}(y-x)^{\top}\nabla^{2}\Phi(\zeta)(y-x)
\]

\end_inset

for some 
\begin_inset Formula $\zeta$
\end_inset

 between 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

.
 Since 
\begin_inset Formula $\nabla^{2}\Phi(\zeta)=\frac{1}{\zeta}$
\end_inset

,
 we have
\begin_inset Formula 
\[
(y-x)^{\top}\nabla^{2}\Phi(\zeta)(y-x)=\sum\frac{(y_{i}-x_{i})^{2}}{\zeta_{i}}\geq\frac{(\sum_{i}|y_{i}-x_{i}|)^{2}}{\sum_{i}\zeta_{i}}=(\sum_{i}|y_{i}-x_{i}|)^{2}
\]

\end_inset

where we used that 
\begin_inset Formula $\sum_{i}x_{i}=\sum_{i}y_{i}=1$
\end_inset

 and so 
\begin_inset Formula $\sum_{i}\zeta_{i}=1$
\end_inset

.
 Hence,
 
\begin_inset Formula 
\[
\Phi(y)-\Phi(x)-\left\langle \nabla\Phi(x),y-x\right\rangle \geq\frac{1}{2}\|y-x\|_{1}^{2}.
\]

\end_inset

Therefore,
 
\begin_inset Formula $\Phi$
\end_inset

 is 
\begin_inset Formula $1$
\end_inset

-strongly convex in 
\begin_inset Formula $\|\cdot\|_{1}$
\end_inset

.
 Hence,
 
\begin_inset Formula $\rho=1$
\end_inset

.
\end_layout

\begin_layout Paragraph
Diameter
\end_layout

\begin_layout Standard
Direct calculation shows that 
\begin_inset Formula $-\log n\leq\Phi(x)\leq0$
\end_inset

.
 We start at 
\begin_inset Formula $x^{(0)}=\frac{1}{n}(1,\ldots,1)^{T}$
\end_inset

.
 Hence,
 
\begin_inset Formula $R^{2}=\log n$
\end_inset

.
\end_layout

\begin_layout Paragraph
Result
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f$
\end_inset

 be a 
\begin_inset Formula $1$
\end_inset

-Lipschitz function on 
\begin_inset Formula $\|\cdot\|_{1}$
\end_inset

.
 Then,
 mirror descent with the mirror map 
\begin_inset Formula $\Phi(x)=\sum_{i}x_{i}\log x_{i}$
\end_inset

.
\begin_inset Formula 
\[
f(x^{T})-\min_{x}f(x)\leq\sqrt{\frac{2\log n}{T}}.
\]

\end_inset


\end_layout

\begin_layout Standard
In comparison,
 projected gradient descent had the bound of 
\begin_inset Formula $\sqrt{\frac{n}{T}}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
make this an exercise...
\end_layout

\begin_layout Subsection
Online Guarantee
\end_layout

\begin_layout Plain Layout
We note that the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:projected_gradient_descent"
nolink "false"

\end_inset

 and Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:mirror_descent"
nolink "false"

\end_inset

 do not use the fact that we are solving the same convex function in each step,
 except for the very last step.
 Suppose at the 
\begin_inset Formula $k$
\end_inset

-th step,
 the function for which we compute the gradient is 
\begin_inset Formula $f^{(k)}$
\end_inset

.
 Then,
 we proved that
\begin_inset Formula 
\begin{equation}
\frac{1}{T}\sum_{k=0}^{T-1}(f^{(k)}(x^{(k)})-f^{(k)}(x))\leq GR\sqrt{\frac{2}{\rho T}}\label{eq:regret}
\end{equation}

\end_inset

for any 
\begin_inset Formula $k$
\end_inset

.
 Also,
 we note that the point 
\begin_inset Formula $x^{(k)}$
\end_inset

 depends on 
\begin_inset Formula $f^{(1)},f^{(2)},\cdots,f^{(k-1)}$
\end_inset

 but not 
\begin_inset Formula $f^{(k)}$
\end_inset

.
 Therefore,
 we can view the whole sequence as a game:
\end_layout

\begin_layout Itemize
The player chooses a point 
\begin_inset Formula $x^{(k)}$
\end_inset

.
\end_layout

\begin_layout Itemize
The adversary reveals 
\begin_inset Formula $\nabla f^{(k)}(x^{(k)})$
\end_inset

.
\end_layout

\begin_layout Itemize
The player gets the loss 
\begin_inset Formula $f^{(k)}(x^{(k)})$
\end_inset

.
\end_layout

\begin_layout Plain Layout
In general,
 we call 
\begin_inset Formula $\sum_{k=0}^{T-1}(f^{(k)}(x^{(k)})-f^{(k)}(x))$
\end_inset

 is the regret of not choosing the best point 
\begin_inset Formula $x$
\end_inset

.
 The equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:regret"
nolink "false"

\end_inset

) shows that mirror descent gives a strategy with regret 
\begin_inset Formula $GR\sqrt{\frac{2T}{\rho}}$
\end_inset

.
 If the domain is the unit ball,
 all functions are 
\begin_inset Formula $1$
\end_inset

-Lipschitz in 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm,
 this shows that a regret bound 
\begin_inset Formula $O(\sqrt{T})$
\end_inset

.
\end_layout

\begin_layout Problem
If the domain is the unit ball,
 all functions are 
\begin_inset Formula $1$
\end_inset

-Lipschitz in 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm and if the adversary reveals only 
\begin_inset Formula $f^{(k)}(x^{(k)})$
\end_inset

,
 what is the best possible regret bound?
 The current best bound 
\begin_inset CommandInset citation
LatexCommand cite
key "bubeck2016kernel"
literal "false"

\end_inset

 is 
\begin_inset Formula $O(n^{c}\sqrt{T})$
\end_inset

 for some pretty large constant 
\begin_inset Formula $c$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cite the new paper
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Warning:
 I am trying to copy the mulicapltive weight update proof.
\end_layout

\begin_layout Plain Layout
But realize that 
\begin_inset Formula $\rho$
\end_inset

 or 
\begin_inset Formula $\rho^{2}$
\end_inset

 dependence is important.
 I am not sure how to recover this for mirror descent.
\end_layout

\begin_layout Plain Layout
(For MWU,
 we have multiplicative error with additive.
 How to talk about this in optimization view?)
\end_layout

\begin_layout Plain Layout
â€”

\end_layout

\begin_layout Plain Layout
As an application,
 we consider the maximum flow problem.
 Note that maxflow problem can be thought as
\begin_inset Formula 
\begin{align*}
 & \min_{f\text{ sends }1\text{ unit from }s\text{ to }t}\max_{e}\frac{f_{e}}{c_{e}}\\
= & \min_{f\text{ sends }1\text{ unit from }s\text{ to }t}\max_{v\in\Delta}\sum_{e}v_{e}\frac{f_{e}}{c_{e}}\\
= & \max_{v\in\Delta}\min_{f\text{ sends }1\text{ unit from }s\text{ to }t}\sum_{e}v_{e}\frac{f_{e}}{c_{e}}.
\end{align*}

\end_inset

Note that the problem 
\begin_inset Formula $F(v)=\min_{f\text{ sends }1\text{ unit from }s\text{ to }t}\sum_{e}v_{e}\frac{f_{e}}{c_{e}}$
\end_inset

 is exactly the shortest path problem with length of 
\begin_inset Formula $e$
\end_inset

 is 
\begin_inset Formula $v_{e}/c_{e}$
\end_inset

.
 The gradient of 
\begin_inset Formula $\min_{f\text{ sends }1\text{ unit from }s\text{ to }t}\sum_{e}v_{e}\frac{f_{e}}{c_{e}}$
\end_inset

 with respect to 
\begin_inset Formula $v$
\end_inset

 is exactly given by 
\begin_inset Formula $f_{e}/c_{e}$
\end_inset

 where 
\begin_inset Formula $f$
\end_inset

 is the shortest path.
 If we apply mirror descent here,
 we recall the guarantee as 
\begin_inset Formula 
\[
F(v)-F^{*}\geq-Lip_{\ell_{1}}\times Diam_{\ell_{1}}\sqrt{\frac{\log m}{T}}.
\]

\end_inset

Note that the diam = 1 and the Lipschitz constant is the sup norm of the gradient.
 In this case,
 we have 
\begin_inset Formula $\frac{1}{\min_{e}c_{e}}$
\end_inset

.
 Hence,
 we have
\begin_inset Formula 
\[
F(v)-F^{*}\geq-\frac{1}{\min_{e}c_{e}}\sqrt{\frac{\log m}{T}}.
\]

\end_inset

However,
 it is not clear how one can get the flow 
\begin_inset Formula $f$
\end_inset

 using 
\begin_inset Formula $v$
\end_inset

.
\end_layout

\begin_layout Plain Layout
It turns out one trick is to expand out the proof (No...I need to use online property here....).
 Note that in the mirror descent proof,
 we in fact show that
\begin_inset Formula 
\[
\frac{1}{T}\sum_{k=1}^{T}F(v^{(k)})\geq F^{*}-\frac{1}{\min_{e}c_{e}}\sqrt{\frac{\log m}{T}}.
\]

\end_inset

Expanding out the definition of 
\begin_inset Formula $F$
\end_inset

,
 we have
\begin_inset Formula 
\[
\frac{1}{T}\sum_{k=1}^{T}\sum_{e\in p^{(k)}}\frac{v_{e}^{(k)}}{c_{e}}\geq\frac{1}{T}\sum_{k=1}^{T}\sum_{e\in p^{(k)}}v_{e}^{(k)}\frac{f_{e}^{(k)}}{c_{e}}\geq F^{*}-\frac{1}{\min_{e}c_{e}}\sqrt{\frac{\log m}{T}}
\]

\end_inset

where 
\begin_inset Formula $p^{(k)}$
\end_inset

 is the shortest path.
 Let 
\begin_inset Formula $f^{*}$
\end_inset

 be the maximum flow.
 Then
\begin_inset Formula 
\[
\frac{1}{\sum_{e\in p^{(k)}}\frac{v_{e}^{(k)}}{c_{e}}}=\frac{\sum_{e}v_{e}^{(k)}}{\sum_{e\in p^{(k)}}\frac{v_{e}^{(k)}}{c_{e}}}\geq\frac{\sum_{e}\frac{v_{e}^{(k)}}{c_{e}}f_{e}^{*}}{\sum_{e\in p^{(k)}}\frac{v_{e}^{(k)}}{c_{e}}}\geq F_{OPT}
\]

\end_inset

where we used 
\begin_inset Formula $p^{(k)}$
\end_inset

 is the shortest path for cost 
\begin_inset Formula $v_{e}/c_{e}$
\end_inset

.
\begin_inset Formula 
\[
\frac{1}{F_{OPT}}\geq\frac{1}{F}-\frac{1}{\min_{e}c_{e}}\sqrt{\frac{\log m}{T}}
\]

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document

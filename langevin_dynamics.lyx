#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass optbook
\use_default_options true
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding auto-legacy
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Standard
Sampling in high dimension is a fundamental problem.
 Informally,
 given access to a function 
\begin_inset Formula $f:\R^{n}\rightarrow\R\cup\left\{ \infty\right\} $
\end_inset

,
 the sampling problem is to generate a point 
\begin_inset Formula $x\in\R^{n}$
\end_inset

 from the distribution with density proportional to 
\begin_inset Formula $e^{-f(x)}$
\end_inset

.
 Note that any density can be written in this form,
 so this is completely general.
 To make the problem precise,
 we also have to specify a starting point with positive density,
 and an error parameter 
\begin_inset Formula $\epsilon$
\end_inset

 that measures the distance of the distribution of the output from the desired target.
 
\end_layout

\begin_layout Standard
Unfortunately,
 in this generality,
 just like optimization,
 sampling is also intractable.
 To see this,
 consider the following function:
\begin_inset Formula 
\[
f(x)=\begin{cases}
0 & x\in S\\
M & x\not\in S
\end{cases}
\]

\end_inset

for some closed set 
\begin_inset Formula $S$
\end_inset

.
 Then sampling according to 
\begin_inset Formula $e^{-f}$
\end_inset

 for a sufficiently large 
\begin_inset Formula $M$
\end_inset

 would allow us to find an element of 
\begin_inset Formula $S$
\end_inset

,
 which could,
 e.g.,
 be the minimizer of a hard-to-optimize function.
\end_layout

\begin_layout Standard
Consider a second example,
 which might appear more tractable:
 
\begin_inset Formula 
\[
g(x)=e^{-\frac{1}{2}x^{\top}Ax}\one_{x\ge0}.
\]

\end_inset

Without the restriction to the nonnegative orthant,
 the target density is the Gaussian 
\begin_inset Formula $N(0,A^{-1})$
\end_inset

,
 and can be sampled by first sampling the standard Gaussian 
\begin_inset Formula $N(0,I)$
\end_inset

 and applying the linear transformation 
\begin_inset Formula $A^{-1/2}$
\end_inset

.
 To sample from the standard Gaussian in 
\begin_inset Formula $\R^{n}$
\end_inset

,
 we can sample each coordinate independently from 
\begin_inset Formula $N(0,1$
\end_inset

),
 a problem which has many (efficient) numerical recipes.
 But how can we handle the restriction?
 In the course of forthcoming chapters,
 we will see that this problem and its generalization to sampling logconcave densities,
 i.e.,
 when 
\begin_inset Formula $f$
\end_inset

 is convex,
 can be solved in polynomial time.
 It is remarkable that the polynomial-time frontier for both optimization and sampling is essentially determined by convexity.
\end_layout

\begin_layout Standard
We begin with gradient-based sampling methods.
 These rely on access to 
\begin_inset Formula $\nabla f$
\end_inset

.
 These methods will in fact be natural algorithmic versions of continuous processes on random variables,
 a particularly pleasing connection.
 Later,
 we will see methods that only use access to 
\begin_inset Formula $f$
\end_inset

,
 and others that utilize higher derivatives,
 notably the Hessian.
 The parallels to optimization will be pervasive and striking.
\end_layout

\begin_layout Section
Gradient-based Sampling:
 Langevin Dynamics
\end_layout

\begin_layout Standard
Here we study a simple stochastic process for generating samples from a desired distribution 
\begin_inset Formula $e^{-f(x)}$
\end_inset

.
 As we will see later in this chapter,
 it can also be viewed as a stochastic version of gradient descent 
\emph on
in the space of measures
\emph default
.
 Gradient descent corresponds to an Ordinary Differential Equation (ODE) for Gradient Flow:
\begin_inset Formula 
\[
dX_{t}=-\nabla f(X_{t})\,dt
\]

\end_inset

which,
 by the chain rule,
 leads to 
\begin_inset Formula 
\[
d(f(X_{t})-f(X_{0}))=\langle\nabla f(X_{t}),dX_{t}\rangle=-\norm{\nabla f(X_{t})}^{2}dt.
\]

\end_inset


\end_layout

\begin_layout Exercise
Show that if 
\begin_inset Formula $f$
\end_inset

 is 
\begin_inset Formula $\mu$
\end_inset

-strongly convex,
 then under Gradient Flow,
 we have 
\begin_inset Formula 
\[
f(X_{t})-f(X^{*})\le e^{-2\mu t}(f(X_{0})-f(X^{*})).
\]

\end_inset


\end_layout

\begin_layout Standard
For sampling from a given starting point,
 we need a process that introduces randomness (since the target itself is a distribution).
 The continuous version of such a process corresponds to a Stochastic Differential Equation (SDE).
 It is called Langevin Diffusion and is the analog of Gradient Flow for optimization.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{algorithm2e}[H]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
caption{
\end_layout

\end_inset


\begin_inset Formula $\mathtt{LangevinDiffusion}$
\end_inset

 (LD)
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
SetAlgoLined
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Input:

\series default
 Initial point 
\begin_inset Formula $x_{0}\in\Rn$
\end_inset

.
\end_layout

\begin_layout Standard
Solve the stochastic differential equation
\begin_inset Formula 
\[
dx_{t}=-\nabla f(x_{t})dt+\sqrt{2}dW_{t}
\]

\end_inset


\series bold
Output:

\series default
 
\begin_inset Formula $x_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{algorithm2e}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $f:\R^{n}\rightarrow\R$
\end_inset

 is a differentiable function,
 
\begin_inset Formula $x_{t}$
\end_inset

 is the random variable at time 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $dW_{t}$
\end_inset

 is infinitesimal Brownian motion also known as a Wiener process.
 We can view it as the limit of the following discrete process 
\begin_inset Formula 
\[
x_{t+1}=x_{t}-h\nabla f(x_{t})+\sqrt{2h}\zeta_{t}
\]

\end_inset

with 
\begin_inset Formula $\zeta_{t}$
\end_inset

 sampled independently from 
\begin_inset Formula $N(0,I)$
\end_inset

.
 When we take the step size 
\begin_inset Formula $h\rightarrow0$
\end_inset

,
 this discrete process converges to the continuous one in distribution.
 We discuss the continuous version first.
\end_layout

\begin_layout Standard
A more general form of an SDE is 
\begin_inset Formula 
\begin{equation}
dx_{t}=\mu(x_{t},t)dt+\sigma(x_{t},t)dW_{t}\label{eq:sde}
\end{equation}

\end_inset

 where 
\begin_inset Formula $x_{t}\in\R^{n},\mu(x_{t},t)\in\Rn$
\end_inset

 is a time-varying vector field and 
\begin_inset Formula $\sigma(x_{t},t)\in\R^{n\times m}$
\end_inset

 is a time-varying linear transformation.
 The simplest such process is the Wiener process:
 
\begin_inset Formula $dx_{t}=dW_{t}$
\end_inset

 which finds many applications in applied mathematics,
 finance,
 biology and physics.
 Another useful process is the Ornstein-Ulhenbeck Process:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dx_{t}=-ax_{t}dt+\sigma dW_{t}
\]

\end_inset

This represents a particle moving in a fluid,
 the first term being the force of friction exerted on the particle and the second term being the movement caused by other particles colliding with our particle,
 which is modeled using Brownian motion.
 
\end_layout

\begin_layout Standard
A crucial difference between ordinary differentials and stochastic differentials is in the chain rule.
 Unlike classical differentials where we have 
\begin_inset Formula $df(x)=\nabla f(x)dx$
\end_inset

,
 we have the following chain rule for stochastic calculus.
\end_layout

\begin_layout Lemma
\begin_inset Argument 1
status open

\begin_layout Plain Layout
It
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
^{o}
\end_layout

\end_inset

's lemma
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lem:itos"

\end_inset

For any process 
\begin_inset Formula $x_{t}\in\Rn$
\end_inset

 satisfying 
\begin_inset Formula $dx_{t}=\mu(x_{t})dt+\sigma(x_{t})dW_{t}$
\end_inset

 where 
\begin_inset Formula $\mu(x_{t})\in\Rn$
\end_inset

 and 
\begin_inset Formula $\sigma(x_{t})\in\R^{n\times m}$
\end_inset

,
 we have that
\begin_inset Formula 
\begin{align*}
df(x_{t}) & =\nabla f(x_{t})^{\top}dx_{t}+\frac{1}{2}(dx_{t})^{\top}\nabla^{2}f(x_{t})(dx_{t})\\
 & =\nabla f(x_{t})^{\top}\mu(x_{t})dt+\nabla f(x_{t})^{\top}\sigma(x_{t})dW_{t}+\frac{1}{2}\tr(\sigma(x_{t})^{\top}\nabla^{2}f(x_{t})\sigma(x_{t}))dt.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The usual chain rule comes from using Taylor expansion and taking a limit,
 i.e.,
 
\begin_inset Formula 
\[
\nabla f(x)=\lim_{h\rightarrow0}\frac{(f(x)+h\nabla f(x)^{\top}y+\frac{1}{2}h^{2}...)-f(x)}{h}=\lim_{h\rightarrow0}\frac{h\nabla f(x)^{\top}y+\frac{1}{2}h^{2}...}{h}
\]

\end_inset

and only the first term survives,
 as the second and later terms goes to zero.
 But with the stochastic component,
 roughly speaking,
 
\begin_inset Formula $(dW_{t})^{2}=dt$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
By definition,
 
\begin_inset Formula $W_{t}\sim N(0,t)$
\end_inset

,
 so the variance of 
\begin_inset Formula $W_{t}$
\end_inset

 is 
\begin_inset Formula $t$
\end_inset

.
 Thus,
 
\begin_inset Formula $(dW_{t})^{2}=dt$
\end_inset

 may seem to make sense.
 This can be justified rigorously through 
\emph on
quadratic variations
\emph default
.
\end_layout

\end_inset

,
 and we need to keep track of the second term in the Taylor expansion as well.
 
\end_layout

\begin_layout Standard
When 
\begin_inset Formula $x_{t}$
\end_inset

 is given by the SDE 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sde"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

,
 its 
\emph on
quadratic variation
\emph default
 
\begin_inset Formula $[x]_{t}$
\end_inset

 is defined as
\begin_inset Formula 
\[
[x]_{t}=\lim_{n\to\infty}\sum_{i=1}^{n}(x_{t_{i}}-x_{t_{i-1}})^{2}=\int_{0}^{t}\sigma_{s}^{2}\,ds,
\]

\end_inset

where 
\begin_inset Formula $\{t_{i}\}_{0\leq i\leq n}$
\end_inset

 is a partition of 
\begin_inset Formula $[0,t]$
\end_inset

 whose mesh size 
\begin_inset Formula $\max_{i\in[n]}|t_{i}-t_{i-1}|$
\end_inset

 goes to zero as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 When 
\begin_inset Formula $x_{t}$
\end_inset

 is vector-valued,
 the quadratic variation is 
\begin_inset Formula 
\[
[x]_{t}=\int_{0}^{t}\sigma_{s}\sigma_{s}^{\T}\,ds\,
\]

\end_inset

and 
\begin_inset Formula $d[X^{i},X^{j}]_{t}=(\sigma_{t}\sigma_{t}^{\top})_{ij}dt$
\end_inset

.
 We can check that 
\begin_inset Formula $[W]_{t}=tI$
\end_inset

.
\end_layout

\begin_layout Standard
For a detailed treatment of stochastic calculus,
 we refer the reader to a standard textbook such as 
\begin_inset CommandInset citation
LatexCommand cite
key "oksendal2013stochastic"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
First,
 we will see that 
\begin_inset Formula $e^{-f}$
\end_inset

 is a stationary density for the Langevin SDE in continuous time.
 The proof relies on the following general theorem about the distribution induced by an SDE.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:fokker_planck"

\end_inset


\begin_inset Argument 1
status open

\begin_layout Plain Layout
Fokker–Planck equation
\end_layout

\end_inset

For any process 
\begin_inset Formula $x_{t}\in\Rn$
\end_inset

 satisfying 
\begin_inset Formula $dx_{t}=\mu(x_{t})dt+\sigma(x_{t})dW_{t}$
\end_inset

 where 
\begin_inset Formula $\mu(x_{t})\in\Rn$
\end_inset

 and 
\begin_inset Formula $\sigma(x_{t})\in\R^{n\times m}$
\end_inset

 with the initial point 
\begin_inset Formula $x_{0}$
\end_inset

 drawn from 
\begin_inset Formula $p_{0}$
\end_inset

.
 Then the density 
\begin_inset Formula $p_{t}$
\end_inset

 of 
\begin_inset Formula $x_{t}$
\end_inset

 satisfies the equation
\begin_inset Formula 
\[
\frac{dp_{t}}{dt}=-\sum_{i}\frac{\partial}{\partial x_{i}}(\mu(x)_{i}p_{t}(x))+\frac{1}{2}\sum_{i,j}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\left[(D(x))_{ij}p_{t}(x)\right]
\]

\end_inset

where 
\begin_inset Formula $D(x)=\sigma(x)\sigma(x)^{\top}$
\end_inset

.
\end_layout

\begin_layout Proof
For any smooth function 
\begin_inset Formula $\phi$
\end_inset

,
 we have that
\begin_inset Formula 
\[
\E_{x\sim p_{t}}\phi(x)=\E\phi(x_{t}).
\]

\end_inset

Taking derivatives on the both sides with respect to 
\begin_inset Formula $t$
\end_inset

,
 using It
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
^{o}
\end_layout

\end_inset

's lemma (Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:itos"
nolink "false"

\end_inset

),
 and noting that 
\begin_inset Formula $\E dW_{t}=0$
\end_inset

,
 we have that
\begin_inset Formula 
\begin{align*}
\int\phi(x)\,dp(x)dx & =\E\left(\nabla\phi(x_{t})^{\top}\mu(x_{t})\,dt+\nabla\phi(x_{t})^{\top}\sigma(x_{t})\,dW_{t}+\frac{1}{2}\tr(\sigma(x_{t})^{\top}\nabla^{2}\phi(x_{t})\sigma(x_{t}))\,dt\right)\\
 & =\E\left(\nabla\phi(x_{t})^{\top}\mu(x_{t})dt+\frac{1}{2}\tr(\nabla^{2}\phi(x_{t})D(x_{t}))\,dt\right).
\end{align*}

\end_inset

Using 
\begin_inset Formula $x_{t}\sim p_{t}$
\end_inset

,
 we have that
\begin_inset Formula 
\[
\int\phi(x)\frac{dp_{t}}{dt}dx=\int\nabla\phi(x)^{\top}\mu(x)p_{t}(x)\,dx+\frac{1}{2}\int\tr(\nabla^{2}\phi(x)D(x))p_{t}(x)\,dx.
\]

\end_inset

Integrating by parts,
 
\begin_inset Formula 
\[
\int\nabla\phi(x)^{\top}\mu(x)p_{t}(x)dx=-\int\phi(x)\sum_{i}\frac{\partial}{\partial x_{i}}(\mu_{i}(x)p_{t}(x))dx.
\]

\end_inset

Similarly,
 integrating by parts twice gives
\begin_inset Formula 
\begin{align*}
\int\tr(\sigma(x)^{\top}\nabla^{2}\phi(x)\sigma(x))p_{t}(x)dx & =\int\tr(\nabla^{2}\phi(x)\sigma(x)\sigma(x)^{\top})p_{t}(x)dx\\
 & =-\int\langle\nabla\phi(x),\sum_{i}\frac{\partial}{\partial x_{i}}(p_{t}(x)D(x)_{i})\rangle dx\\
 & =\sum_{i,j}\int\phi(x)\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\left[(D(x))_{ij}p_{t}(x)\right]dx.
\end{align*}

\end_inset

Hence,
 
\begin_inset Formula 
\[
\int\phi(x)\left[\frac{dp_{t}}{dt}+\sum_{i}\frac{\partial}{\partial x_{i}}(\mu(x)_{i}p_{t}(x))-\frac{1}{2}\sum_{i,j}\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\left[(D(x))_{ij}p_{t}(x)\right]\right]dx=0
\]

\end_inset

for any smooth 
\begin_inset Formula $\phi$
\end_inset

.
 Therefore,
 we have the conclusion of the theorem.
\end_layout

\begin_layout Proof
We apply the Fokker–Planck equation to the Langevin dynamics.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:langevin_stationary"

\end_inset

For any smooth function 
\begin_inset Formula $f$
\end_inset

,
 the density proportional to 
\begin_inset Formula $F=e^{-f}$
\end_inset

 is stationary for the Langevin dynamics.
\end_layout

\begin_layout Proof
The Fokker–Planck equation (Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:fokker_planck"
nolink "false"

\end_inset

) shows that the distribution 
\begin_inset Formula $p_{t}$
\end_inset

 of 
\begin_inset Formula $x_{t}$
\end_inset

 satisfies 
\begin_inset Formula 
\begin{equation}
\frac{dp_{t}}{dt}=\sum_{i}\frac{\partial}{\partial x_{i}}(\frac{\partial f(x)}{\partial x_{i}}p_{t}(x))+\sum_{i}\frac{\partial^{2}}{\partial x_{i}^{2}}\left[p_{t}(x)\right].\label{eq:dp_LD}
\end{equation}

\end_inset

Now since 
\begin_inset Formula $p_{t}$
\end_inset

 is stationary the LHS is zero and we can rewrite the above as
\begin_inset Formula 
\begin{align*}
\frac{dp_{t}}{dt}=0 & =\sum_{i}\frac{\partial}{\partial x_{i}}\left(p_{t}(x)\frac{\partial f(x)}{\partial x_{i}}+\frac{\partial}{\partial x_{i}}p_{t}(x)\right)\\
 & =\sum_{i}\frac{\partial}{\partial x_{i}}\left(p_{t}(x)\left(\frac{\partial f(x)}{\partial x_{i}}+\frac{\partial}{\partial x_{i}}\log p_{t}(x)\right)\right)\\
 & =\sum_{i}\frac{\partial}{\partial x_{i}}\left(p_{t}(x)\left(\frac{\partial}{\partial x_{i}}\left[\log\frac{p_{t}(x)}{e^{-f(x)}}\right]\right)\right).
\end{align*}

\end_inset

We can verify that 
\begin_inset Formula $p_{t}(x)\propto e^{-f(x)}$
\end_inset

 is a solution.
\end_layout

\begin_layout Exercise
Consider the equation 
\begin_inset Formula 
\[
\frac{d}{dt}p_{t}(x)=\frac{1}{2}\frac{d^{2}}{dx^{2}}p_{t}(x)
\]

\end_inset

 for 
\begin_inset Formula $x\in\R,t>0$
\end_inset

.
 Show that the density of 
\begin_inset Formula $\mathcal{N}(0,t)$
\end_inset

,
 i.e.
 
\begin_inset Formula 
\[
\phi_{t}(x)=\frac{1}{\sqrt{2\pi t}}\exp\left(-\frac{x^{2}}{2t}\right)
\]

\end_inset

 satisfies this equation.
 Then generalize the process to 
\begin_inset Formula $\R^{n}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Exercise
Consider the SDE with 
\begin_inset Formula $X_{t}\in\R^{n}$
\end_inset

:
 
\begin_inset Formula 
\[
dX_{t}=-X_{t}dt+\sqrt{2}dW_{t}.
\]

\end_inset

Use the Fokker-Planck equation to derive a corresponding stationary density.
 Use It
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
^{o}
\end_layout

\end_inset

's lemma to derive 
\begin_inset Formula $\E(\norm{X_{t}}_{4}^{4})$
\end_inset

.
 (Hint:
 Take expectation on both sides of It
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
^{o}
\end_layout

\end_inset

's lemma for appropriate 
\begin_inset Formula $f(X_{t})$
\end_inset

,
 and use 
\begin_inset Formula $\E df(X_{t})=d\E f(X_{t})$
\end_inset

 for continuous 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $df$
\end_inset

).
 
\end_layout

\begin_layout Subsection
Convergence via Coupling in Wasserstein distance
\end_layout

\begin_layout Standard
Next we turn to the rate of convergence,
 which will also prove uniqueness of the stationary distribution for the stochastic process.
 For this,
 we assume that 
\begin_inset Formula $f$
\end_inset

 is strongly convex.
 The proof is via the classical coupling technique 
\begin_inset CommandInset citation
LatexCommand cite
key "aldous1995reversible"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Our goal is to bound the rate at which the distribution of the current point approaches the stationary distribution,
 in some chosen measure of distance between distributions (for example,
 the TV distance).
 To do this,
 in the coupling technique,
 we consider two points which are both following the random process.
 One of them is already in the stationary distribution,
 and therefore will stay there.
 The other is our point.
 We will show that there is a coupling of the two distributions,
 i.e.,
 a joint distribution over the two points,
 whose marginals are identical to the single point processes,
 such that the expected distance between the two points decreases at a certain rate.
 More formally,
 we couple two copies 
\begin_inset Formula $x_{t},y_{t}$
\end_inset

 of the random process with different starting points (the coupling is a joint distribution 
\begin_inset Formula $D(x_{t},y_{t})$
\end_inset

 with the property that its marginal for each of 
\begin_inset Formula $x_{t},y_{t}$
\end_inset

 is exactly the process) and show that their distributions get closer over time.
 
\end_layout

\begin_layout Standard
While the challenge usually is to find a good coupling,
 in the present case,
 the simple identity coupling (i.e.,
 the same Wiener process is used for both 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}$
\end_inset

) works well.
 The distance measure we will use here is the Wasserstein distance (in Euclidean norm,
 see Definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "defn:The-Wasserstein--distance"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

).
 
\end_layout

\begin_layout Exercise
Show that for two distributions with the same finite support,
 computing their Wasserstein distance reduces to a bipartite matching problem.
 
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $x_{t},y_{t}$
\end_inset

 evolve according to the Langevin diffusion for a 
\begin_inset Formula $\mu$
\end_inset

-strongly convex function 
\begin_inset Formula $f:\R^{n}\rightarrow\R$
\end_inset

.
 Then,
 there is a coupling 
\begin_inset Formula $\gamma$
\end_inset

 between 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}$
\end_inset

 s.t.
\begin_inset Formula 
\[
\E_{x_{t},y_{t}\sim\gamma}\norm{x_{t}-y_{t}}^{2}\le e^{-2\mu t}\norm{x_{0}-y_{0}}^{2}.
\]

\end_inset


\end_layout

\begin_layout Proof
From the definition of LD,
 and by using the identity coupling,
 i.e.,
 the same Gaussian 
\begin_inset Formula $dW_{t}$
\end_inset

 for both processes 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}$
\end_inset

,
 we have that 
\begin_inset Formula 
\[
\frac{d}{dt}\left(x_{t}-y_{t}\right)=\nabla f(y_{t})-\nabla f(x_{t}).
\]

\end_inset

Hence,
 
\begin_inset Formula 
\[
\frac{1}{2}\frac{d}{dt}\|x_{t}-y_{t}\|^{2}=\left\langle \nabla f(y_{t})-\nabla f(x_{t}),x_{t}-y_{t}\right\rangle .
\]

\end_inset

Next,
 from the strong convexity of 
\begin_inset Formula $f$
\end_inset

,
 we have 
\begin_inset Formula 
\begin{align*}
f(y_{t})-f(x_{t}) & \ge\nabla f(x_{t})^{\top}(y_{t}-x_{t})+\frac{\mu}{2}\norm{x_{t}-y_{t}}^{2},\\
f(x_{t})-f(y_{t}) & \ge\nabla f(y_{t})^{\top}(x_{t}-y_{t})+\frac{\mu}{2}\norm{x_{t}-y_{t}}^{2}.
\end{align*}

\end_inset

Adding two equations together,
 we have
\begin_inset Formula 
\[
(\nabla f(x_{t})-\nabla f(y_{t}))^{\top}(x_{t}-y_{t})\geq\mu\norm{x_{t}-y_{t}}^{2}.
\]

\end_inset

Therefore,
 
\begin_inset Formula 
\[
\frac{1}{2}\frac{d}{dt}\|x_{t}-y_{t}\|^{2}\leq-\mu\|x_{t}-y_{t}\|^{2}.
\]

\end_inset

 Hence,
 
\begin_inset Formula 
\begin{align*}
\frac{d}{dt}\log\|x_{t}-y_{t}\|^{2} & =\frac{\frac{d}{dt}\|x_{t}-y_{t}\|^{2}}{\|x_{t}-y_{t}\|^{2}}\\
 & \leq-2\mu.
\end{align*}

\end_inset

Integrating both sides from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $t$
\end_inset

,
 we get 
\begin_inset Formula 
\[
\log\|x_{t}-y_{t}\|^{2}-\log\|x_{0}-y_{0}\|^{2}\le-2\mu t
\]

\end_inset

which proves the result.
\end_layout

\begin_layout Exercise
Give an example of a function 
\begin_inset Formula $f$
\end_inset

 for which the density proportional to 
\begin_inset Formula $e^{-f}$
\end_inset

 is 
\emph on
not
\emph default
 stationary for the following discretized Langevin algorithm 
\begin_inset Formula 
\[
x^{(k+1)}=x^{(k)}-\epsilon\nabla f(x^{(k)})+\sqrt{2\epsilon}Z^{(k)}
\]

\end_inset

where 
\begin_inset Formula $Z^{(k)}\sim\mathcal{N}(0,1)$
\end_inset

 are independent and the distribution of 
\begin_inset Formula $x^{(0)}$
\end_inset

 is Gaussian.
\end_layout

\begin_layout Subsection
Convergence in 
\begin_inset Formula $\chi^{2}$
\end_inset

 and KL divergences
\end_layout

\begin_layout Standard
Next we analyze the convergence of Langevin dynamics in the two most commonly used divergence metrics.
 The rate of mixing will depend on a fundamental functional constants of the target density,
 which we define next.
 These constants will play an important role in later algorithms and analysis,
 and we will also see equivalent geometric formulations of them.
\end_layout

\begin_layout Definition
We say that a probability measure 
\begin_inset Formula $\nu$
\end_inset

 on 
\begin_inset Formula $\R^{n}$
\end_inset

 satisfies a 
\emph on
Poincar
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
'e
\end_layout

\end_inset

 inequality
\emph default
 (PI) with parameter 
\begin_inset Formula $C_{PI}(\nu)$
\end_inset

 if for all smooth functions 
\begin_inset Formula $g:\R^{n}\to\R$
\end_inset

,
\begin_inset Formula 
\begin{equation}
\Var_{\nu}g\leq C_{PI}(\nu)\,\E_{\nu}\left(\norm{\nabla g}^{2}\right)\label{eq:pi}
\end{equation}

\end_inset

where 
\begin_inset Formula $\Var_{\nu}g=\E_{\nu}\abs{g-\E_{\pi}g}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The Poincar
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
'e
\end_layout

\end_inset

 inequality is implied by the generally stronger log-Sobolev inequality.
\end_layout

\begin_layout Definition
We say that a probability measure 
\begin_inset Formula $\nu$
\end_inset

 on 
\begin_inset Formula $\R^{n}$
\end_inset

 satisfies a 
\emph on
log-Sobolev inequality
\emph default
 (LSI) with parameter 
\begin_inset Formula $C_{LSI}(\nu)$
\end_inset

 if for all smooth functions 
\begin_inset Formula $g:\Rn\to\R$
\end_inset

,
\begin_inset Formula 
\begin{equation}
\Ent_{\nu}(g^{2})\leq2C_{LSI}(\nu)\,\E_{\nu}\left(\norm{\nabla g}^{2}\right)\label{eq:lsi}
\end{equation}

\end_inset

where 
\begin_inset Formula $\Ent_{\nu}(g^{2}):=\E_{\nu}(g^{2}\log g^{2})-\E_{\nu}(g^{2})\log\E_{\nu}(g^{2})$
\end_inset

.
 
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:LDinKL"

\end_inset

Let 
\begin_inset Formula $f$
\end_inset

 be a smooth function with log-Sobolev constant 
\begin_inset Formula $C_{LSI}$
\end_inset

.
 Then the Langevin dynamics 
\begin_inset Formula 
\[
dx_{t}=-\nabla f(x)dt+\sqrt{2}dW_{t}
\]

\end_inset

converges exponentially in KL-divergence (Definition
\begin_inset ERT
status open

\begin_layout Plain Layout

~
\end_layout

\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "defn:KL-divergence"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

) to the density 
\begin_inset Formula $\nu(x)\propto e^{-f(x)}$
\end_inset

 with mixing rate 
\begin_inset Formula $O(C_{LSI})$
\end_inset

,
 i.e.,
 the distribution 
\begin_inset Formula $\rho_{t}$
\end_inset

of 
\begin_inset Formula $x_{t}$
\end_inset

satisfies 
\begin_inset Formula 
\[
d_{KL}(\rho_{t},\nu)\le e^{-\frac{2}{C_{LSI}}t}d_{KL}(\rho_{0},\nu).
\]

\end_inset


\end_layout

\begin_layout Standard
The theorem will follow from the following lemma and the log-Sobolev inequality.
\end_layout

\begin_layout Lemma
For a target distribution 
\begin_inset Formula $\nu$
\end_inset

,
 with 
\begin_inset Formula $\rho_{t}$
\end_inset

 being the distribution attained by Langevin dynamics at time 
\begin_inset Formula $t$
\end_inset

,
 starting at 
\begin_inset Formula $\rho_{0}$
\end_inset

,
 we have
\begin_inset Formula 
\[
\frac{d}{dt}d_{KL}(\rho_{t},\nu)=-J_{\nu}(\rho_{t})
\]

\end_inset

where 
\begin_inset Formula $J_{\nu}(\rho)$
\end_inset

 is the relative Fisher information of 
\begin_inset Formula $\rho$
\end_inset

 wrt 
\begin_inset Formula $\nu$
\end_inset

,
 defined as 
\begin_inset Formula $\E_{\rho}(\norm{\nabla\log(\rho/\nu)}^{2})$
\end_inset

.
\end_layout

\begin_layout Proof
We compute the time derivative of the distribution of LD at time 
\begin_inset Formula $t$
\end_inset

 with 
\begin_inset Formula $\rho=\rho_{t}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\frac{d}{dt}d_{KL}(\rho_{t},\nu) & =\frac{d}{dt}\int_{\R^{n}}\rho\log\frac{\rho}{\nu}\\
 & =\int\frac{d\rho}{dt}\left(\log\frac{\rho}{\nu}+1\right)\\
 & =\int\nabla\cdot\left(\rho\nabla\log\frac{\rho}{\nu}\right)\log\frac{\rho}{\nu}\\
 & =-\int\langle\rho\nabla\log\frac{\rho}{\nu},\nabla\log\frac{\rho}{\nu}\rangle\\
 & =-\int\rho\norm{\nabla\log\frac{\rho}{\nu}}_{2}^{2}\\
 & =\E_{\rho}\left(\norm{\nabla\log\frac{\rho}{\nu}}_{2}^{2}\right)=-J_{\nu}(\rho_{t}).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The next lemma shows that the log-Sobolev constant can be equivalently formulated in terms of distributions in place of smooth functions.
\end_layout

\begin_layout Lemma
The log-Sobolev constant of a distribution 
\begin_inset Formula $\nu$
\end_inset

 on 
\begin_inset Formula $\R^{n}$
\end_inset

can be equivalently defined as follows:
 for any distribution 
\begin_inset Formula $\rho$
\end_inset

 on 
\begin_inset Formula $\R^{n}$
\end_inset

,
 we have
\begin_inset Formula 
\[
d_{KL}(\rho,\nu)\le\frac{C_{LSI}}{2}J_{\nu}(\rho).
\]

\end_inset


\end_layout

\begin_layout Proof
To go one way,
 we use 
\begin_inset Formula $g=\sqrt{\frac{\rho}{\nu}};$
\end_inset

to go the other way,
 define 
\begin_inset Formula $\rho=\frac{g^{2}\nu}{\E_{\nu}(g^{2})}$
\end_inset

.
\end_layout

\begin_layout Standard
We are now ready to prove Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:LDinKL"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Proof
We use Lemmas and to obtain
\begin_inset Formula 
\[
\frac{d}{dt}d_{KL}(\rho_{t},\nu)=-J_{\nu}(\rho_{t})\le-\frac{2}{C_{LSI}}d_{KL}(\rho_{t},\nu).
\]

\end_inset

Integrating over time gives the theorem.
\end_layout

\begin_layout Exercise
Show that for Langevin Dynamics applied to a target density 
\begin_inset Formula $\nu$
\end_inset

 with Poincar
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
'e
\end_layout

\end_inset

 constant 
\begin_inset Formula $C_{PI}$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\chi^{2}(\rho_{t},\nu)\le e^{-\frac{2}{C_{PI}}t}\chi^{2}(\rho_{0},\nu).
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
section{Langevin Dynamics is Gradient Descent in Density Space*
\backslash
footnote{Sections marked with * are more mathematical and can be skipped.}}
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sec:LD_GD"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Langevin Dynamics is Gradient Descent in Density Space*
\begin_inset Foot
status open

\begin_layout Plain Layout
Sections marked with * are more mathematical and can be skipped.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here we show that Langevin dynamics is simply gradient descent for the function 
\begin_inset Formula $F(\rho)=\KL(\rho\|\nu)$
\end_inset

 for two densities 
\begin_inset Formula $\rho,\nu$
\end_inset

 on the Wasserstein space where 
\begin_inset Formula $\nu=e^{-f(x)}/\int e^{-f(y)}dy$
\end_inset

 is the target and 
\begin_inset Formula $\rho$
\end_inset

 is the current density.
 For this,
 we first define the Wasserstein space.
\end_layout

\begin_layout Definition
The Wasserstein space 
\begin_inset Formula $P_{2}(\Rn)$
\end_inset

 on 
\begin_inset Formula $\Rn$
\end_inset

 is the manifold on the set of probability measures on 
\begin_inset Formula $\Rn$
\end_inset

 such that the shortest path distance of two measures 
\begin_inset Formula $x,y$
\end_inset

 in this manifold is exactly equal to the Wasserstein distance between 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Definition
We let 
\begin_inset Formula $T_{p}({\cal M})$
\end_inset

 refer to the tangent space at a point 
\begin_inset Formula $p$
\end_inset

 in a manifold 
\begin_inset Formula ${\cal M}.$
\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:local-norm"

\end_inset

For any 
\begin_inset Formula $p\in P_{2}(\Rn)$
\end_inset

 and 
\begin_inset Formula $v\in T_{p}P_{2}(\Rn)$
\end_inset

,
 we can write 
\begin_inset Formula $v(x)=\nabla\cdot(p(x)\nabla\lambda(x))$
\end_inset

 for some function 
\begin_inset Formula $\lambda$
\end_inset

 on 
\begin_inset Formula $\Rn$
\end_inset

.
 Furthermore,
 the local norm of 
\begin_inset Formula $v$
\end_inset

 in this metric is given by
\begin_inset Formula 
\[
\|v\|_{p}^{2}=\E_{x\sim p}\|\nabla\lambda(x)\|^{2}.
\]

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $p\in P_{2}(\Rn)$
\end_inset

 and 
\begin_inset Formula $v\in T_{p}P_{2}(\Rn)$
\end_inset

.
 We will show that any change of density 
\begin_inset Formula $v$
\end_inset

 can be represented by a vector field 
\begin_inset Formula $c$
\end_inset

 on 
\begin_inset Formula $\Rn$
\end_inset

 as follows:
 Consider the process 
\begin_inset Formula $x_{0}\sim p$
\end_inset

 and 
\begin_inset Formula $\frac{d}{dt}x_{t}=c(x_{t})$
\end_inset

.
 Let 
\begin_inset Formula $p_{t}$
\end_inset

 be the density of the distribution of 
\begin_inset Formula $x_{t}$
\end_inset

.
 To compute 
\begin_inset Formula $\frac{d}{dt}p_{t}$
\end_inset

,
 we follow the same idea as in the proof as Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:fokker_planck"
nolink "false"

\end_inset

.
 For any smooth function 
\begin_inset Formula $\phi$
\end_inset

,
 we have that 
\begin_inset Formula $\E_{x\sim p_{t}}\phi(x)=\E\phi(x_{t}).$
\end_inset

 Taking derivatives on the both sides with respect to 
\begin_inset Formula $t$
\end_inset

,
 we have that
\begin_inset Formula 
\[
\int\phi(x)\frac{d}{dt}p_{t}(x)dx=\int\nabla\phi(x)^{\top}c(x)p_{t}(x)dx=-\int\nabla\cdot(c(x)p_{t}(x))\phi(x)dx
\]

\end_inset

where we used integration by parts at the end.
 Since this holds for all 
\begin_inset Formula $\phi$
\end_inset

,
 we have that
\begin_inset Formula 
\[
\frac{dp_{t}(x)}{dt}=-\nabla\cdot(p_{t}(x)c(x)).
\]

\end_inset

Since we are interested only in vector fields that generate the minimum movement in Wasserstein distance,
 we consider the optimization problem
\begin_inset Formula 
\[
\min_{-\nabla\cdot(pc)=v}\frac{1}{2}\int p(x)\|c(x)\|^{2}dx
\]

\end_inset

where we can think 
\begin_inset Formula $v$
\end_inset

 is the change of 
\begin_inset Formula $p_{t}$
\end_inset

.
 Let 
\begin_inset Formula $\lambda(x)$
\end_inset

 be the Lagrangian multiplier of the constraint 
\begin_inset Formula $-\nabla\cdot(pc)=v$
\end_inset

.
 Then,
 the problem becomes
\begin_inset Formula 
\begin{align*}
 & \min_{c}\frac{1}{2}\int p(x)\|c(x)\|^{2}dx-\int\lambda(x)\nabla\cdot(p(x)c(x))dx.\\
= & \min_{c}\frac{1}{2}\int p(x)\|c(x)\|^{2}dx+\int\nabla\lambda(x)^{\top}c(x)\cdot p(x)dx.
\end{align*}

\end_inset

Now,
 we note that the problem is a pointwise optimization problem whose minimizer is given by
\begin_inset Formula 
\[
c(x)=-\nabla\lambda(x).
\]

\end_inset

This proves that any vector field that generates minimum movement in Wasserstein distance is a gradient field.
 Also,
 we have that 
\begin_inset Formula $v(x)=\nabla\cdot(p(x)\nabla\lambda(x)).$
\end_inset

 Note that the right hand side is an elliptical differential equation and hence for any 
\begin_inset Formula $v$
\end_inset

 with 
\begin_inset Formula $\int v(x)dx=0$
\end_inset

,
 there is an unique solution 
\begin_inset Formula $\lambda(x)$
\end_inset

.
 Therefore,
 we can write 
\begin_inset Formula $v(x)=\nabla\cdot(p(x)\nabla\lambda(x))$
\end_inset

 for some 
\begin_inset Formula $\lambda(x)$
\end_inset

.
\end_layout

\begin_layout Proof
Next,
 we note that the movement is given by
\begin_inset Formula 
\[
\|v\|_{p}^{2}=\int p(x)\|c(x)\|^{2}dx=\E_{x\sim p}\|\nabla\lambda(x)\|^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
As we discussed in the gradient descent section,
 one can use norms other than 
\begin_inset Formula $\ell_{2}$
\end_inset

 norm.
 For the Wasserstein space,
 we should use the local norm as given in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:local-norm"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:LD_GD"

\end_inset

Let 
\begin_inset Formula $\rho_{t}$
\end_inset

 be the density of the distribution produced by Langevin Dynamics for the target distribution 
\begin_inset Formula $\nu=e^{-f(x)}/\int e^{-f(y)}dy$
\end_inset

.
 Then,
 we have that
\begin_inset Formula 
\[
\frac{d\rho}{dt}=\argmin_{v\in T_{p}P_{2}(\Rn)}\left\langle \nabla F(\rho),v\right\rangle _{p}+\frac{1}{2}\|v\|_{p}^{2}.
\]

\end_inset

Namely,
 
\begin_inset Formula $\rho_{t}$
\end_inset

 follows continuous gradient descent in the density space for the function 
\begin_inset Formula $F(\rho)=\KL(\rho\|\nu)$
\end_inset

 under the Wasserstein metric.
\end_layout

\begin_layout Proof
For any function 
\begin_inset Formula $c$
\end_inset

,
 the optimization problem of interest satisfies
\begin_inset Formula 
\[
\min_{\delta=\nabla\cdot(\rho\nabla\lambda)}\left\langle c,\delta\right\rangle +\frac{1}{2}\int\rho(x)\|\nabla\lambda(x)\|^{2}dx=\min_{\nabla\lambda}-\int\rho(x)\cdot\nabla c(x)^{\top}\nabla\lambda(x)dx+\frac{1}{2}\int\rho(x)\|\nabla\lambda(x)\|^{2}dx.
\]

\end_inset

Solving the right hand side,
 we have 
\begin_inset Formula $\nabla c=\nabla\lambda$
\end_inset

 and hence 
\begin_inset Formula $\delta=\nabla\cdot(\rho\nabla c)$
\end_inset

.
 Now,
 we note that 
\begin_inset Formula $\nabla F(\rho)=\log\frac{\rho}{\nu}-1$
\end_inset

.
 Therefore,
 
\begin_inset Formula 
\begin{align*}
\frac{d\rho}{dt} & =\nabla\cdot(\rho\nabla(\log\frac{\rho}{\nu}-1))\\
 & =\nabla\cdot(\rho\nabla\log\frac{\rho}{\nu})\\
 & =\nabla\cdot\left(\rho\nabla f\right)+\Delta\rho
\end{align*}

\end_inset

which is exactly equal to (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dp_LD"
nolink "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
To analyze this continuous descent in Wasserstein space,
 we first prove that continuous gradient descent converges exponentially whenever 
\begin_inset Formula $F$
\end_inset

 is strongly convex.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:grad-dom"

\end_inset

Let 
\begin_inset Formula $F$
\end_inset

 be a function satisfying 
\begin_inset Quotes eld
\end_inset

Gradient Dominance
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{equation}
\norm{\nabla F(x)}_{x}^{2}\ge\alpha\cdot(F(x)-\min_{y}F(y))\quad\text{for all }x\label{eq:LD_strong_convexity}
\end{equation}

\end_inset

on the manifold with the metric 
\begin_inset Formula $\|\cdot\|_{x}$
\end_inset

 where 
\begin_inset Formula $\nabla$
\end_inset

 is the gradient on the manifold.
 Then,
 the process 
\begin_inset Formula $dx_{t}=-\nabla F(x_{t})dt$
\end_inset

 converges exponentially,
 i.e.,
 
\begin_inset Formula $F(x_{t})-\min_{y}F(y)\le e^{-\alpha t}(F(x_{0})-\min_{y}F(y))$
\end_inset

.
\end_layout

\begin_layout Proof
We write
\begin_inset Formula 
\[
\frac{d}{dt}(F(x)-\min_{y}F(y))=\langle\nabla F(x_{t}),\frac{dx_{t}}{dt}\rangle_{x_{t}}=-\|\nabla F(x_{t})\|_{x_{t}}^{2}\leq-\alpha(F(x)-\min_{y}F(y)).
\]

\end_inset

The conclusion follows.
\end_layout

\begin_layout Standard
Finally,
 we note that the log-Sobolev inequality for the density 
\begin_inset Formula $\nu$
\end_inset

 can be re-stated as the condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LD_strong_convexity"
nolink "false"

\end_inset

).
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:log_sob_LD"

\end_inset

Fix a density 
\begin_inset Formula $\nu$
\end_inset

.
 Then the log-Sobolev inequality,
 namely,
 for every smooth function 
\begin_inset Formula $g$
\end_inset

,
 
\begin_inset Formula 
\[
\frac{C_{LSI}}{2}\int\norm{\nabla g}^{2}\,d\nu\ge\int g(x)^{2}\log g(x)^{2}\ d\nu
\]

\end_inset

 implies the condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LD_strong_convexity"
nolink "false"

\end_inset

) with 
\begin_inset Formula $\alpha=1/C_{LSI}$
\end_inset

.
\end_layout

\begin_layout Proof
Take 
\begin_inset Formula $g(x)=\sqrt{\frac{\rho(x)}{\nu(x)}}$
\end_inset

,
 the log-Sobolev inequality shows that 
\begin_inset Formula 
\[
\frac{C_{LSI}}{2}\int\rho(x)\norm{\nabla\log\frac{\rho(x)}{\nu(x)}}^{2}\,dx\geq\int\rho(x)\log\frac{\rho(x)}{\nu(x)}\,dx\text{ for all }\rho.
\]

\end_inset


\end_layout

\begin_layout Proof
As we calculate in Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:LD_GD"
nolink "false"

\end_inset

,
 we have that
\begin_inset Formula 
\[
\norm{\nabla F(\rho)}_{\rho}^{2}=\int\rho(x)\norm{\nabla\log\frac{\rho(x)}{\nu(x)}}^{2}\,dx.
\]

\end_inset

Therefore,
 this is exactly the condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LD_strong_convexity"
nolink "false"

\end_inset

) with coefficient 
\begin_inset Formula $2/C_{LSI}$
\end_inset

.
\end_layout

\begin_layout Standard
Combining Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:log_sob_LD"
nolink "false"

\end_inset

 and Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:grad-dom"
nolink "false"

\end_inset

,
 we obtain another proof of Therem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:LDinKL"
plural "false"
caps "false"
noprefix "false"
nolink "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Standard
Langevin dynamics converges quickly in continuous time for isoperimetric distributions.
 Turning this into an efficient algorithm typically needs more assumptions and there is much room for choosing discretizations.
 This is similar to the situation with gradient descent for optimization.
 As we saw in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:LD_GD"
nolink "false"

\end_inset

,
 it turns out that Langevin dynamics is in fact gradient descent in the space of probability measures under the Wasserstein metric,
 where the function being minimized is the KL-divergence of the current density from the target stationary density.
 For more on this view of sampling as optimization over measures,
 see 
\begin_inset CommandInset citation
LatexCommand cite
key "wibisono2018sampling"
literal "true"

\end_inset

.
 See 
\begin_inset CommandInset citation
LatexCommand cite
key "lee2024eldan"
literal "true"

\end_inset

 for a tight estimate of log-Sobolev constant for logconcave measures.
 In particular for a logconcave measure with support of diameter 
\begin_inset Formula $D$
\end_inset

,
 the log-Sobolev constant is 
\begin_inset Formula $\Omega(1/D)$
\end_inset

.
\end_layout

\end_body
\end_document
